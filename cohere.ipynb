{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \\\n",
    "#   langchain_community \\\n",
    "#   langchain_pinecone \\\n",
    "#   langchain_openai \\\n",
    "#   unstructured \\\n",
    "#   langchain-text-splitters \\\n",
    "#   pinecone-text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujjwal/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ujjwal/Library/Python/3.9/lib/python/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import (\n",
    "    PineconeHybridSearchRetriever)\n",
    "#from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "#from langchain.vectorstores import PineconeVectorStore  \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.runnables import Runnable\n",
    "from pinecone import ServerlessSpec\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pinecone\n",
    "import uuid\n",
    "import os\n",
    "import glob\n",
    "import getpass\n",
    "import hashlib\n",
    "from tqdm.autonotebook import tqdm\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pinecone import Pinecone\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import nltk\n",
    "#  nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: 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\n",
      "Pinecone API Key: 45944250-e820-42d1-8914-0da88c671dda\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "# Fetch API keys from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "\n",
    "# Set the environment variables\n",
    "if openai_api_key:\n",
    "    os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ['PINECONE_API_KEY'] = pinecone_api_key \n",
    "\n",
    "#Verify that the keys are loaded\n",
    "print(f\"OpenAI API Key: {os.environ.get('OPENAI_API_KEY')}\")\n",
    "print(f\"Pinecone API Key: {os.environ.get('PINECONE_API_KEY')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data'\n",
    "\n",
    "def load_docs(directory):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "docs = load_docs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x3522cf2e0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x355bc5340>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"HUGGINGFACEHUB_API_TOKEN\"\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",  #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    ")\n",
    "embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "import time\n",
    "\n",
    "index_name = \"test-3\"  # change if desired\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "#pc.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 500 \n",
    "chunk_overlap = 50  \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"test-3\"\n",
    "\n",
    "vectorstore = PineconeVectorStore.from_documents(split_docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x355bc80d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'how to make a new case? ',\n",
       " 'result': 'To make a new case, follow these steps:\\n\\n1. Go to the Cases module from the left menu bar.\\n2. Click on the Create button.\\n3. Create a case for an existing client by typing in the name of the client you want to create a case for.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "\n",
    ")\n",
    "qa.invoke(\"how to make a new case? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an expert LLM assistant specialized in answering questions based solely on the information provided in the uploaded documents (PDF, DOCX, or TXT formats). Use only the information from the documents to respond accurately and clearly to each question.\n",
    "\n",
    "Guidelines:\n",
    "1. Avoid using outside knowledge or assumptions. Stick strictly to the content in the documents.\n",
    "2. If the answer is not found in the uploaded documents, state, \"The answer is not specifically mentioned in the provided documents.\"\n",
    "3. Avoid using outside knowledge or assumptions. Stick strictly to the content in the documents.\n",
    "4. Maintain a professional and helpful tone thinking you are giving service to the customer for their documents.\n",
    "5. Answer for normal conversation questions like \"Hi\", \"Hey\", \"Hello\", \"How are you?\", and many others with the answer: \"Hello, How can I assist you?\".\n",
    "6. If the question is on \"summarize\" or \"summarization\", then summarize the documents to (1/4)th the size of the original documents.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Describe case modules\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With and Without Knowledge Base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with knowledge:\n",
      "The Cases module is a feature that allows you to manage and organize cases for clients. You can create a new case for an existing client by typing in the client's name. The module also provides functionality to filter and find cases based on their status. Additionally, you can navigate to a specific case and add tasks to it by clicking on the Task tab and then selecting Add Task.\n",
      "\n",
      "Chat without knowledge:\n",
      "\"Case modules\" can refer to a variety of concepts depending on the context in which they are used. Here are a few interpretations:\n",
      "\n",
      "1. **Educational Case Modules**: In an educational setting, case modules are often used as teaching tools. They present real-world scenarios or problems that students must analyze and solve. These modules are commonly used in business, law, medicine, and other professional fields to help students apply theoretical knowledge to practical situations.\n",
      "\n",
      "2. **Software Development**: In software development, case modules might refer to components or units of a larger system that handle specific tasks or functions. These modules can be part of a modular design approach, where a system is divided into separate, interchangeable components that can be developed and tested independently.\n",
      "\n",
      "3. **Legal Case Modules**: In the legal field, case modules could refer to structured outlines or summaries of legal cases used for study or reference. These modules might include key facts, legal issues, arguments, and decisions, helping law students or professionals quickly understand and analyze cases.\n",
      "\n",
      "4. **Product Design and Manufacturing**: In product design, case modules might refer to the physical enclosures or housings of electronic devices or machinery. These modules are designed to protect internal components and provide a user interface.\n",
      "\n",
      "5. **Healthcare and Medical Training**: In healthcare, case modules are often used in training programs to simulate patient scenarios. These modules help medical students and professionals practice diagnostic and treatment skills in a controlled environment.\n",
      "\n",
      "If you have a specific context in mind, please provide more details so I can give a more precise explanation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Chat with knowledge:\")\n",
    "print(qa.invoke(query1).get(\"result\"))\n",
    "print(\"\\nChat without knowledge:\")\n",
    "print(llm.invoke(query1).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"test-3\",\n",
       "    \"dimension\": 1536,\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"test-3-unx28qm.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"deletion_protection\": \"enabled\"\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.describe_index(\"test-3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete by embeddedId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ids in index.list(prefix='7EB541E4-91A9-4DEB-BB7E-55813D3CA140#'):\n",
    "#     print(ids)\n",
    "#     index.delete(ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: pinecone-client[grpc]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone-client[grpc]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by tenantId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '179f0723-cbf2-4f83-82a5-f5dc509870a3#chunk46',\n",
       "              'metadata': {'fileName': 'Cases Module ✓.docx',\n",
       "                           'tenantId': 'khadka'},\n",
       "              'score': -0.022306226,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []},\n",
       "             {'id': '179f0723-cbf2-4f83-82a5-f5dc509870a3#chunk21',\n",
       "              'metadata': {'fileName': 'Cases Module ✓.docx',\n",
       "                           'tenantId': 'khadka'},\n",
       "              'score': -0.023610583,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []},\n",
       "             {'id': '179f0723-cbf2-4f83-82a5-f5dc509870a3#chunk60',\n",
       "              'metadata': {'fileName': 'Cases Module ✓.docx',\n",
       "                           'tenantId': 'khadka'},\n",
       "              'score': -0.024194181,\n",
       "              'sparse_values': {'indices': [], 'values': []},\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "index_name = \"test-3\"\n",
    "query = \"how to make a new case?\"\n",
    "query_vector = [0.1] * 1536 \n",
    "\n",
    "response = index.query(\n",
    "    namespace=\"\",\n",
    "    vector=query_vector,\n",
    "    filter={\n",
    "        \"tenantId\": {\"$eq\": \"khadka\"}  \n",
    "    },\n",
    "    top_k=3,\n",
    "    include_metadata=True  \n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 179f0723-cbf2-4f83-82a5-f5dc509870a3#chunk46\n",
      "score: -0.022306226\n",
      "metadata: {'fileName': 'Cases Module ✓.docx', 'tenantId': 'khadka'}\n",
      "id: 179f0723-cbf2-4f83-82a5-f5dc509870a3#chunk21\n",
      "score: -0.023610583\n",
      "metadata: {'fileName': 'Cases Module ✓.docx', 'tenantId': 'khadka'}\n",
      "id: 179f0723-cbf2-4f83-82a5-f5dc509870a3#chunk60\n",
      "score: -0.024194181\n",
      "metadata: {'fileName': 'Cases Module ✓.docx', 'tenantId': 'khadka'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = index.query(\n",
    "        namespace=\"\",\n",
    "        vector=query_vector,\n",
    "        filter={\"tenantId\": {\"$eq\": \"khadka\"}},\n",
    "        top_k=3,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    # Check if there are any matches\n",
    "    if response.matches:\n",
    "        for match in response.matches:\n",
    "            print(\"id:\", match.id)\n",
    "            print(\"score:\", match.score)\n",
    "            print(\"metadata:\", match.metadata)\n",
    "    else:\n",
    "        print(\"Sorry, No matches found with this tenantId. \")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error querying the index:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by tenantId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `text` key. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'query': 'How to make a new case?', 'response': {'matches': [{'id': 'a129771a-5db6-4d57-b462-b1b6045198c5#chunk30',\n",
      "              'metadata': {'fileName': '_Billings Module ✓.docx',\n",
      "                           'tenantId': 'ujjwal'},\n",
      "              'score': -0.02252464,\n",
      "              'sparse_values': {'indices': [], 'values': []},\n",
      "              'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 6}}, 'result': 'To make a new case, follow these steps:\\n\\n1. Go to the Cases module from the left menu bar.\\n2. Click on the Create button.\\n3. If you are creating a case for an existing client, type in the name of the client you want to create a case for.\\n4. If you are creating a case for a new patient, use the Add Patient button.\\n5. Click Next.\\n6. Add a service (optional).\\n7. Click Save.\\n\\nPlease note that only cases assigned to you will be shown by default under the cases module.'}\n"
     ]
    }
   ],
   "source": [
    "query5 = \"How to make a new case?\"\n",
    "query_vector = [0.1] * 1536  \n",
    "\n",
    "try:\n",
    "    response = index.query(\n",
    "        namespace=\"\",\n",
    "        vector=query_vector,\n",
    "        filter={\"tenantId\": {\"$eq\": \"ujjwal\"}},\n",
    "        top_k=1,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    if response.matches:\n",
    "        match_data = response.matches[0].metadata\n",
    "        \n",
    "        context = f\"tenantId: {match_data.get('tenantId', '')}\"\n",
    "        \n",
    "        answer = qa.invoke({\"query\": query, \"response\": response})\n",
    "        \n",
    "        print(\"Answer:\", answer)\n",
    "    else:\n",
    "        print(\"No matches found for tenantId \")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error querying the index:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.02: ujjwal\n"
     ]
    }
   ],
   "source": [
    "for match in response['matches']:\n",
    "    print(f\"{match['score']:.2f}: {match['metadata']['tenantId']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 3750\n",
    "\n",
    "def retrieve(query):\n",
    "    response = OpenAI.Embedding.create(\n",
    "        input=[query],\n",
    "        #engine=embeddings\n",
    "    )\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = response['data'][0]['embedding']\n",
    "\n",
    "    # get relevant contexts\n",
    "    response = index.query(xq, top_k=3, include_metadata=True)\n",
    "    contexts = [\n",
    "        x['metadata']['text'] for x in response['matches']\n",
    "    ]\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Embedding",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow to make a new case?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m query_with_contexts \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m query_with_contexts\n",
      "Cell \u001b[0;32mIn[161], line 4\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(query):\n\u001b[0;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m[query],\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m#engine=embeddings\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# retrieve from Pinecone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     xq \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pydantic/_internal/_model_construction.py:262\u001b[0m, in \u001b[0;36mModelMetaclass.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m private_attributes \u001b[38;5;129;01mand\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m private_attributes:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m private_attributes[item]\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(item)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Embedding"
     ]
    }
   ],
   "source": [
    "query5 = \"How to make a new case?\"\n",
    "query_with_contexts = retrieve(query5)\n",
    "query_with_contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
