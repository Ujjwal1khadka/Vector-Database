{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \\\n",
    "#   langchain_community \\\n",
    "#   langchain_pinecone \\\n",
    "#   langchain_openai \\\n",
    "#   unstructured \\\n",
    "#   langchain-text-splitters \\\n",
    "#   pinecone-text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import (\n",
    "    PineconeHybridSearchRetriever)\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "#from langchain.vectorstores import PineconeVectorStore  \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.runnables import Runnable\n",
    "from pinecone import ServerlessSpec\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pinecone\n",
    "import uuid\n",
    "import os\n",
    "import glob\n",
    "import hashlib\n",
    "from tqdm.autonotebook import tqdm\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import nltk\n",
    "#  nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Fetch API keys from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "\n",
    "# Set the environment variables\n",
    "if openai_api_key:\n",
    "    os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ['PINECONE_API_KEY'] = pinecone_api_key \n",
    "\n",
    "#Verify that the keys are loaded\n",
    "#print(f\"OpenAI API Key: {os.environ.get('OPENAI_API_KEY')}\")\n",
    "#print(f\"Pinecone API Key: {os.environ.get('PINECONE_API_KEY')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data'\n",
    "\n",
    "def load_docs(directory):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "docs = load_docs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs[0].page_content[0:100])\n",
    "# print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x32464f920>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x32478a270>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"HUGGINGFACEHUB_API_TOKEN\"\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",  #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    ")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import time\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "import time\n",
    "\n",
    "index_name = \"test-2\"  # change if desired\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "#pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader  # Assuming DirectoryLoader is part of LangChain\n",
    "\n",
    "# directory = 'data'\n",
    "\n",
    "# # Function to load documents from the directory\n",
    "# def load_docs(directory):\n",
    "#     loader = DirectoryLoader(directory)\n",
    "#     docs = loader.load()  # Load documents using the DirectoryLoader\n",
    "#     return docs\n",
    "\n",
    "# # Upload documents and store metadata including source and text\n",
    "# def upload_documents(docs, embeddings, index):\n",
    "#     for doc in docs:\n",
    "#         # Extract document content and metadata\n",
    "#         document_text = doc.page_content\n",
    "#         source = doc.metadata.get('source', 'unknown')  # Assuming 'source' is part of doc.metadata\n",
    "\n",
    "#         if document_text:  # Only proceed if the document was read successfully\n",
    "#             # 1. Generate embedding for the document\n",
    "#             vector = embeddings.embed_documents([document_text])[0]  # Use embed_documents for document embeddings\n",
    "\n",
    "#             # 2. Generate a unique vector ID (UUID)\n",
    "#             vector_id = str(uuid.uuid4())  # Generate a unique UUID\n",
    "            \n",
    "#             # 3. Prepare metadata (source and content preview)\n",
    "#             metadata = {\n",
    "#                 \"source\": source,  # The document name as source\n",
    "#                 \"text\": document_text[:100]  # Store first 100 characters as a preview of the document\n",
    "#             }\n",
    "            \n",
    "#             # 4. Upsert the vector into Pinecone with metadata\n",
    "#             index.upsert(vectors=[(vector_id, vector, metadata)])  # Make sure to include metadata here\n",
    "\n",
    "#             # 5. Print vector ID and document name\n",
    "#             print(f\"Uploaded Document - Vector ID: {vector_id}, Document Source: {source}\")\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Load documents using load_docs function\n",
    "# docs = load_docs(directory)\n",
    "\n",
    "# # Call the function to upload documents into Pinecone vector store\n",
    "# upload_documents(docs, embeddings, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader  # Assuming DirectoryLoader is part of LangChain\n",
    "\n",
    "# directory = 'data'\n",
    "\n",
    "# # Function to load documents from the directory\n",
    "# def load_docs(directory):\n",
    "#     loader = DirectoryLoader(directory)\n",
    "#     docs = loader.load()  # Load documents using the DirectoryLoader\n",
    "#     return docs\n",
    "\n",
    "# # Upload documents and store metadata including source and text\n",
    "# def upload_documents(docs, embeddings, index, tenant_id):\n",
    "#     for doc in docs:\n",
    "#         # Extract document content and metadata\n",
    "#         document_text = doc.page_content\n",
    "#         source = doc.metadata.get('source', 'unknown')  # Assuming 'source' is part of doc.metadata\n",
    "\n",
    "#         if document_text:  # Only proceed if the document was read successfully\n",
    "#             # 1. Generate embedding for the document\n",
    "#             vector = embeddings.embed_documents([document_text])[0]  # Use embed_documents for document embeddings\n",
    "\n",
    "#             # 2. Generate a unique vector ID (UUID)\n",
    "#             vector_id = str(uuid.uuid4())  # Generate a unique UUID\n",
    "            \n",
    "#             # 3. Prepare metadata (source, tenantId, and content preview)\n",
    "#             metadata = {\n",
    "#                 \"source\": source,  # The document name as source\n",
    "#                 \"tenantId\": tenantId,  # Add tenant-specific ID\n",
    "#                 \"text\": document_text[:100]  # Store first 100 characters as a preview of the document\n",
    "#             }\n",
    "            \n",
    "#             # 4. Upsert the vector into Pinecone with metadata\n",
    "#             index.upsert(vectors=[{\"id\": vector_id, \"values\": vector, \"metadata\": metadata}])  # Include metadata here\n",
    "\n",
    "#             # 5. Print vector ID and document name\n",
    "#             print(f\"Uploaded Document - Vector ID: {vector_id}, Document Source: {source}, Tenant ID: {tenantId}\")\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Load documents using load_docs function\n",
    "# docs = load_docs(directory)\n",
    "\n",
    "# # Example of embedding generation and Pinecone index setup\n",
    "# # embeddings and index should already be initialized\n",
    "# tenantId = \"example_tenantId\"  # Add your tenantId here\n",
    "\n",
    "# # Call the function to upload documents into Pinecone vector store\n",
    "# upload_documents(docs, embeddings, index, tenantId)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader  # Assuming DirectoryLoader is part of LangChain\n",
    "\n",
    "# directory = 'data'\n",
    "\n",
    "# # Function to load documents from the directory\n",
    "# def load_docs(directory):\n",
    "#     loader = DirectoryLoader(directory)\n",
    "#     docs = loader.load()  # Load documents using the DirectoryLoader\n",
    "#     return docs\n",
    "\n",
    "# # Upload documents and store metadata including source and text\n",
    "# def upload_documents(docs, embeddings, index):\n",
    "#     for doc in docs:\n",
    "#         # Extract document content and metadata\n",
    "#         document_text = doc.page_content\n",
    "#         source = doc.metadata.get('source', 'unknown')  # Assuming 'source' is part of doc.metadata\n",
    "\n",
    "#         if document_text:  # Only proceed if the document was read successfully\n",
    "#             # 1. Generate embedding for the document\n",
    "#             vector = embeddings.embed_documents([document_text])[0]  # Use embed_documents for document embeddings\n",
    "\n",
    "#             # 2. Generate a unique vector ID (UUID)\n",
    "#             vector_id = str(uuid.uuid4())  # Generate a unique UUID\n",
    "            \n",
    "#             # 3. Prepare metadata (source and content preview)\n",
    "#             metadata = {\n",
    "#                 \"source\": source,  # The document name as source\n",
    "#                 \"text\": document_text[:100]  # Store first 100 characters as a preview of the document\n",
    "#             }\n",
    "            \n",
    "#             # 4. Upsert the vector into Pinecone with metadata\n",
    "#             index.upsert(vectors=[{\"id\": vector_id, \"values\": vector, \"metadata\": metadata}])  # Include metadata here\n",
    "\n",
    "#             # 5. Print vector ID and document name\n",
    "#             print(f\"Uploaded Document - Vector ID: {vector_id}, Document Source: {source}\")\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Load documents using load_docs function\n",
    "# docs = load_docs(directory)\n",
    "\n",
    "# # Example of embedding generation and Pinecone index setup\n",
    "# # embeddings and index should already be initialized\n",
    "\n",
    "# # Call the function to upload documents into Pinecone vector store\n",
    "# upload_documents(docs, embeddings, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "# directory = 'data'\n",
    "# def load_docs(directory):\n",
    "#     loader = DirectoryLoader(directory)\n",
    "#     docs = loader.load()  # Load documents using the DirectoryLoader\n",
    "#     return docs\n",
    "\n",
    "# chunk_size = 1000  # Adjust based on your use case\n",
    "# chunk_overlap = 100  # Adjust based on your use case\n",
    "\n",
    "# # Function to read text from different file types\n",
    "# def read_document(file_path):\n",
    "#     if file_path.endswith('.txt'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     elif file_path.endswith('.pdf'):\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             reader = PdfReader(file)\n",
    "#             return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "#     elif file_path.endswith('.docx'):\n",
    "#         doc = docx.Document(file_path)\n",
    "#         return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "#     else:\n",
    "#         return None  # Unsupported file type\n",
    "\n",
    "# # Function to upload document chunks with ID prefixes\n",
    "# def upload_documents(directory, embeddings, index):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "#     # Loop through files in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         document_text = read_document(file_path)\n",
    "        \n",
    "#         if document_text:\n",
    "#             # Generate the document ID prefix\n",
    "#             doc_id_prefix = f\"{filename.split('.')[0]}\"\n",
    "\n",
    "#             # Split the document into chunks\n",
    "#             chunks = text_splitter.split_text(document_text)\n",
    "            \n",
    "#             vectors = []\n",
    "#             for i, chunk in enumerate(chunks):\n",
    "#                 # Create unique ID for each chunk\n",
    "#                 chunk_id = f\"{doc_id_prefix}#chunk{i+1}\"\n",
    "#                 # Generate embedding for each chunk\n",
    "#                 vector = embeddings.embed_documents([chunk])[0]\n",
    "#                 # Prepare metadata (source and content preview)\n",
    "#                 metadata = {\n",
    "#                     \"source\": filename,\n",
    "#                     \"page_content\": chunk[:100],  # Store first 100 characters as a preview of the chunk\n",
    "#                 }\n",
    "#                 # Append vector with ID and metadata\n",
    "#                 vectors.append({\"id\": chunk_id, \"values\": vector, \"metadata\": metadata})\n",
    "\n",
    "#             # Upsert the vectors (chunks) into Pinecone with metadata\n",
    "#             index.upsert(vectors=vectors, namespace=\"\")\n",
    "\n",
    "#             print(f\"Uploaded Document - Prefix: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Call the function to upload documents from the directory\n",
    "# upload_documents(directory, embeddings, index)\n",
    "\n",
    "# # Function to list all chunks of a document by prefix\n",
    "# def list_document_chunks(doc_id_prefix):\n",
    "#     chunk_ids = []\n",
    "#     for ids in index.list(prefix=f\"{doc_id_prefix}#\", namespace=\"\"):\n",
    "#         chunk_ids.extend(ids)\n",
    "#     return chunk_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# import random\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# directory = 'data'\n",
    "# def load_docs(directory):\n",
    "#     loader = DirectoryLoader(directory)\n",
    "#     docs = loader.load()  # Load documents using the DirectoryLoader\n",
    "#     return docs\n",
    "\n",
    "# chunk_size = 1000  # Adjust based on your use case\n",
    "# chunk_overlap = 100  # Adjust based on your use case\n",
    "\n",
    "# # Function to read text from different file types\n",
    "# def read_document(file_path):\n",
    "#     if file_path.endswith('.txt'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     elif file_path.endswith('.pdf'):\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             reader = PdfReader(file)\n",
    "#             return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "#     elif file_path.endswith('.docx'):\n",
    "#         doc = docx.Document(file_path)\n",
    "#         return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "#     else:\n",
    "#         return None  # Unsupported file type\n",
    "\n",
    "# # Function to generate a random 8-digit ID\n",
    "# def generate_random_id():\n",
    "#     return str(random.randint(10000000, 99999999))\n",
    "\n",
    "# # Function to upload document chunks with a random 8-digit ID prefix\n",
    "# def upload_documents(directory, embeddings, index):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "#     # Loop through files in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         document_text = read_document(file_path)\n",
    "        \n",
    "#         if document_text:\n",
    "#             # Generate a random 8-digit document ID prefix\n",
    "#             doc_id_prefix = generate_random_id()\n",
    "\n",
    "#             # Split the document into chunks\n",
    "#             chunks = text_splitter.split_text(document_text)\n",
    "            \n",
    "#             vectors = []\n",
    "#             for i, chunk in enumerate(chunks):\n",
    "#                 # Create unique ID for each chunk\n",
    "#                 chunk_id = f\"{doc_id_prefix}#chunk{i+1}\"\n",
    "#                 # Generate embedding for each chunk\n",
    "#                 vector = embeddings.embed_documents([chunk])[0]\n",
    "#                 # Prepare metadata (source and content preview)\n",
    "#                 metadata = {\n",
    "#                     \"source\": filename,\n",
    "#                     \"page_content\": chunk[:100],  # Store first 100 characters as a preview of the chunk\n",
    "#                 }\n",
    "#                 # Append vector with ID and metadata\n",
    "#                 vectors.append({\"id\": chunk_id, \"values\": vector, \"metadata\": metadata})\n",
    "\n",
    "#             # Upsert the vectors (chunks) into Pinecone with metadata\n",
    "#             index.upsert(vectors=vectors, namespace=\"\")\n",
    "\n",
    "#             print(f\"Uploaded Document - Prefix: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Function to list all chunks of a document by prefix\n",
    "# def list_document_chunks(doc_id_prefix):\n",
    "#     chunk_ids = []\n",
    "#     for ids in index.list(prefix=f\"{doc_id_prefix}#\", namespace=\"\"):\n",
    "#         chunk_ids.extend(ids)\n",
    "#     return chunk_ids\n",
    "\n",
    "# # Call the function to upload documents from the directory\n",
    "# upload_documents(directory, embeddings, index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping unsupported or empty file: data/.DS_Store\n",
      "Uploaded Document - Prefix: world war, Total Chunks: 15\n",
      "All documents uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "from PyPDF2 import PdfReader  # For reading PDF files\n",
    "import docx\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "directory = 'data'\n",
    "chunk_size = 1000  # Adjust based on your use case\n",
    "chunk_overlap = 100  # Adjust based on your use case\n",
    "batch_size = 50  # Number of chunks per batch for upsert to Pinecone\n",
    "\n",
    "\n",
    "# Function to read text from different file types\n",
    "def read_document(file_path):\n",
    "    if file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PdfReader(file)\n",
    "            return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "    elif file_path.endswith('.docx'):\n",
    "        doc = docx.Document(file_path)\n",
    "        return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "    else:\n",
    "        return None  # Unsupported file type\n",
    "\n",
    "\n",
    "# Function to process and upsert a single document\n",
    "def process_and_upload_document(file_path, embeddings, index, tenantId):\n",
    "    document_text = read_document(file_path)\n",
    "\n",
    "    if document_text:\n",
    "        # Generate the document ID prefix\n",
    "        doc_id_prefix = f\"{os.path.basename(file_path).split('.')[0]}\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "        # Split the document into chunks\n",
    "        chunks = text_splitter.split_text(document_text)\n",
    "        vectors = []\n",
    "        \n",
    "        # Create vectors for each chunk\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"{doc_id_prefix}#chunk{i+1}\"\n",
    "            vector = embeddings.embed_documents([chunk])[0]\n",
    "            metadata = {\n",
    "                \"filename\": os.path.basename(file_path),\n",
    "                \"tenantId\": tenantId,  # Add tenant_ID to metadata\n",
    "            }\n",
    "            vectors.append({\"id\": chunk_id, \"values\": vector, \"metadata\": metadata})\n",
    "\n",
    "            # Batch upsert after processing `batch_size` chunks\n",
    "            if len(vectors) >= batch_size:\n",
    "                index.upsert(vectors=vectors, namespace=\"\")\n",
    "                vectors.clear()  # Clear the batch after uploading\n",
    "\n",
    "        # Upsert any remaining vectors\n",
    "        if vectors:\n",
    "            index.upsert(vectors=vectors, namespace=\"\")\n",
    "        \n",
    "        print(f\"Uploaded Document - Prefix: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "    else:\n",
    "        print(f\"Skipping unsupported or empty file: {file_path}\")\n",
    "\n",
    "\n",
    "# Function to upload documents in parallel\n",
    "def upload_documents_concurrently(directory, embeddings, index, tenantId):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            futures.append(executor.submit(process_and_upload_document, file_path, embeddings, index, tenantId))\n",
    "        \n",
    "        # Wait for all threads to complete\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "    print(\"All documents uploaded successfully!\")\n",
    "\n",
    "\n",
    "# Call the function to upload documents from the directory concurrently\n",
    "tenantId = \"your_tenantId_here\"  # Add tenant ID here or pass dynamically\n",
    "upload_documents_concurrently(directory, embeddings, index, tenantId)\n",
    "\n",
    "\n",
    "# Function to list all chunks of a document by prefix\n",
    "def list_document_chunks(doc_id_prefix):\n",
    "    chunk_ids = []\n",
    "    for ids in index.list(prefix=f\"{doc_id_prefix}#\", namespace=\"\"):\n",
    "        chunk_ids.extend(ids)\n",
    "    return chunk_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName: world war.pdf, tenantId: Vitafy, embeddedId: 8a981444-0737-4449-a273-8288ba9309eb\n",
      "All documents uploaded successfully! Total number of chunks inserted to Pinecone: 15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import uuid  # For generating unique IDs\n",
    "from PyPDF2 import PdfReader  # For reading PDF files\n",
    "import docx\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "directory = 'data'\n",
    "chunk_size = 1000  # Adjust based on your use case\n",
    "chunk_overlap = 100  # Adjust based on your use case\n",
    "batch_size = 50  # Number of chunks per batch for upsert to Pinecone\n",
    "\n",
    "# Function to read text from different file types\n",
    "def read_document(file_path):\n",
    "    if file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PdfReader(file)\n",
    "            return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "    elif file_path.endswith('.docx'):\n",
    "        doc = docx.Document(file_path)\n",
    "        return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "    else:\n",
    "        return None  # Unsupported file type\n",
    "\n",
    "# Function to process and upsert a single document\n",
    "def process_and_upload_document(file_path, embeddings, index, tenantId):\n",
    "    document_text = read_document(file_path)\n",
    "\n",
    "    if document_text:\n",
    "        # Generate a unique ID for the entire document\n",
    "        embedded_id = str(uuid.uuid4())\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "        # Split the document into chunks\n",
    "        chunks = text_splitter.split_text(document_text)\n",
    "        vectors = []\n",
    "        \n",
    "        # Create vectors for each chunk using the same embedded_id\n",
    "        for chunk in chunks:\n",
    "            vector = embeddings.embed_documents([chunk])[0]\n",
    "            metadata = {\n",
    "                \"filename\": os.path.basename(file_path),\n",
    "                \"tenantId\": tenantId,  # Add tenant_ID to metadata\n",
    "            }\n",
    "            vectors.append({\"id\": embedded_id, \"values\": vector, \"metadata\": metadata})\n",
    "\n",
    "            # Batch upsert after processing `batch_size` chunks\n",
    "            if len(vectors) >= batch_size:\n",
    "                index.upsert(vectors=vectors, namespace=\"\")\n",
    "                vectors.clear()  # Clear the batch after uploading\n",
    "\n",
    "        # Upsert any remaining vectors\n",
    "        if vectors:\n",
    "            index.upsert(vectors=vectors, namespace=\"\")\n",
    "        \n",
    "        # Print the filename, tenant ID, and the shared embedded ID for the document once after all chunks are uploaded\n",
    "        print(f\"fileName: {os.path.basename(file_path)}, tenantId: {tenantId}, embeddedId: {embedded_id}\")\n",
    "        \n",
    "        # Return the total number of chunks for this document\n",
    "        return len(chunks)\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping unsupported or empty file: {file_path}\")\n",
    "        return 0  # Return 0 if no valid document was processed\n",
    "\n",
    "def upload_documents_concurrently(directory, embeddings, index, tenantId):\n",
    "    total_chunks = 0  # Counter for total chunks across all documents\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for filename in os.listdir(directory):\n",
    "            # Skip unsupported or empty files\n",
    "            if filename == \".DS_Store\":\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            futures.append(executor.submit(process_and_upload_document, file_path, embeddings, index, tenantId))\n",
    "        \n",
    "        # Wait for all threads to complete\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            total_chunks += future.result()  # Aggregate the total chunks\n",
    "\n",
    "    print(f\"All documents uploaded successfully! Total number of chunks inserted to Pinecone: {total_chunks}\")\n",
    "\n",
    "# Call the function to upload documents from the directory concurrently\n",
    "tenantId = \"Vitafy\"  # Add tenant ID here or pass dynamically\n",
    "upload_documents_concurrently(directory, embeddings, index, tenantId)\n",
    "\n",
    "# Function to list all chunks of a document by embedded ID\n",
    "def list_document_chunks(embeddedId):\n",
    "    chunk_ids = []\n",
    "    for ids in index.list(prefix=f\"{embeddedId}\", namespace=\"\"):\n",
    "        chunk_ids.extend(ids)\n",
    "    return chunk_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No chunks found for document: Settings Module\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "# Function to list all chunks of a document by prefix\n",
    "def list_document_chunks(doc_id_prefix):\n",
    "    chunk_ids = []\n",
    "    # Assuming index.list supports pagination or has a mechanism to retrieve chunks efficiently\n",
    "    for ids in index.list(prefix=f\"{doc_id_prefix}#\", namespace=\"\"):\n",
    "        chunk_ids.extend(ids)\n",
    "    return chunk_ids\n",
    "\n",
    "\n",
    "# Function to delete all chunks of a document by prefix in parallel\n",
    "def delete_document_by_prefix(doc_id_prefix, batch_size=100):\n",
    "    chunk_ids = list_document_chunks(doc_id_prefix)\n",
    "    \n",
    "    if chunk_ids:\n",
    "        def delete_chunks_batch(batch):\n",
    "            index.delete(ids=batch, namespace=\"\")\n",
    "        \n",
    "        # Batch the chunk deletions to reduce overhead\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Split chunk_ids into batches of size `batch_size`\n",
    "            batches = [chunk_ids[i:i + batch_size] for i in range(0, len(chunk_ids), batch_size)]\n",
    "            futures = [executor.submit(delete_chunks_batch, batch) for batch in batches]\n",
    "\n",
    "            # Wait for all batch deletions to complete\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                future.result()\n",
    "\n",
    "        print(f\"Deleted {len(chunk_ids)} chunks for document: {doc_id_prefix}\")\n",
    "    else:\n",
    "        print(f\"No chunks found for document: {doc_id_prefix}\")\n",
    "\n",
    "\n",
    "# Function to upload documents in parallel (unchanged)\n",
    "def upload_documents_concurrently(directory, embeddings, index, tenant_ID):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            futures.append(executor.submit(process_and_upload_document, file_path, embeddings, index, tenantId))\n",
    "        \n",
    "        # Wait for all threads to complete\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "    print(\"All documents uploaded successfully!\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "doc_id_prefix = \"Settings Module\"  # Replace with your actual document name\n",
    "\n",
    "# Delete all chunks for a document in parallel\n",
    "delete_document_by_prefix(doc_id_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# import random\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # directory = 'data'\n",
    "# chunk_size = 1000  # Adjust based on your use case\n",
    "# chunk_overlap = 100  # Adjust based on your use case\n",
    "\n",
    "# # # Function to load documents\n",
    "# # def load_docs(directory):\n",
    "# #     loader = DirectoryLoader(directory)\n",
    "# #     docs = loader.load()  # Load documents using the DirectoryLoader\n",
    "# #     return docs\n",
    "\n",
    "# # Function to read text from different file types\n",
    "# def read_document(file_path):\n",
    "#     if file_path.endswith('.txt'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     elif file_path.endswith('.pdf'):\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             reader = PdfReader(file)\n",
    "#             return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "#     elif file_path.endswith('.docx'):\n",
    "#         doc = docx.Document(file_path)\n",
    "#         return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "#     else:\n",
    "#         return None  # Unsupported file type\n",
    "\n",
    "# # Function to generate a 14-digit random ID\n",
    "# def generate_14_digit_random_id():\n",
    "#     return str(random.randint(10000000000000, 99999999999999))\n",
    "\n",
    "# # Function to upload document chunks with a random 14-digit ID prefix\n",
    "# def upload_documents(directory, embeddings, index, tenant_ID):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "#     # Loop through files in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         document_text = read_document(file_path)\n",
    "        \n",
    "#         if document_text:\n",
    "#             # Generate a random 14-digit document ID prefix\n",
    "#             doc_id_prefix = generate_14_digit_random_id()\n",
    "\n",
    "#             # Split the document into chunks\n",
    "#             chunks = text_splitter.split_text(document_text)\n",
    "            \n",
    "#             vectors = []\n",
    "#             for i, chunk in enumerate(chunks):\n",
    "#                 # Create unique ID for each chunk\n",
    "#                 chunk_id = f\"{doc_id_prefix}#chunk{i+1}\"\n",
    "#                 # Generate embedding for each chunk\n",
    "#                 vector = embeddings.embed_documents([chunk])[0]\n",
    "#                 # Prepare metadata (source, tenant_ID, and content preview)\n",
    "#                 metadata = {\n",
    "#                     \"source\": filename,\n",
    "#                     \"tenant_ID\": tenant_ID,  # Store the tenant ID as metadata\n",
    "#                     \"page_content\": chunk[:100],  # Store first 100 characters as a preview of the chunk\n",
    "#                 }\n",
    "#                 # Append vector with ID and metadata\n",
    "#                 vectors.append({\"id\": chunk_id, \"values\": vector, \"metadata\": metadata})\n",
    "\n",
    "#             # Upsert the vectors (chunks) into Pinecone with metadata\n",
    "#             index.upsert(vectors=vectors, namespace=\"\")\n",
    "\n",
    "#             #print(f\"Uploaded Document - Prefix: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "#             print(f\"Uploaded Document - Filename: {filename}, embedded_ID: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Function to list all chunks of a document by prefix\n",
    "# def list_document_chunks(doc_id_prefix):\n",
    "#     chunk_ids = []\n",
    "#     for ids in index.list(prefix=f\"{doc_id_prefix}#\", namespace=\"\"):\n",
    "#         chunk_ids.extend(ids)\n",
    "#     return chunk_ids\n",
    "\n",
    "# # Example call to upload documents from the directory\n",
    "# tenant_ID = \"tenant1234\"  # Example tenant ID\n",
    "# upload_documents(directory, embeddings, index, tenant_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# # Define constants\n",
    "# chunk_size = 1000  # Adjust based on your use case\n",
    "# chunk_overlap = 100  # Adjust based on your use case\n",
    "\n",
    "# # Function to read text from different file types\n",
    "# def read_document(file_path):\n",
    "#     if file_path.endswith('.txt'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     elif file_path.endswith('.pdf'):\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             reader = PdfReader(file)\n",
    "#             return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "#     elif file_path.endswith('.docx'):\n",
    "#         doc = docx.Document(file_path)\n",
    "#         return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "#     else:\n",
    "#         return None  # Unsupported file type\n",
    "\n",
    "# # Function to generate a 14-digit random ID\n",
    "# def generate_14_digit_random_id():\n",
    "#     return str(random.randint(10000000000000, 99999999999999))\n",
    "\n",
    "# # Function to process each document\n",
    "# def process_document(filename, directory, embeddings, index, tenant_ID):\n",
    "#     file_path = os.path.join(directory, filename)\n",
    "#     document_text = read_document(file_path)\n",
    "    \n",
    "#     if document_text:\n",
    "#         # Generate a random 14-digit document ID prefix\n",
    "#         doc_id_prefix = generate_14_digit_random_id()\n",
    "\n",
    "#         # Split the document into chunks\n",
    "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "#         chunks = text_splitter.split_text(document_text)\n",
    "\n",
    "#         # Generate embeddings for chunks in batches\n",
    "#         vectors = []\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             chunk_id = f\"{doc_id_prefix}#chunk{i+1}\"\n",
    "#             vectors.append({\"id\": chunk_id, \"content\": chunk})\n",
    "\n",
    "#         # Generate embeddings for all chunks at once\n",
    "#         if vectors:\n",
    "#             content_list = [vector[\"content\"] for vector in vectors]\n",
    "#             embedded_vectors = embeddings.embed_documents(content_list)\n",
    "\n",
    "#             # Prepare the vectors for upsert\n",
    "#             for vector, embedding in zip(vectors, embedded_vectors):\n",
    "#                 vector[\"values\"] = embedding\n",
    "#                 vector[\"metadata\"] = {\n",
    "#                     \"source\": filename,\n",
    "#                     \"tenant_ID\": tenant_ID,\n",
    "#                     \"page_content\": vector[\"content\"][:100],  # Store first 100 characters as a preview\n",
    "#                 }\n",
    "\n",
    "#             # Upsert the vectors (chunks) into Pinecone with metadata\n",
    "#             index.upsert(vectors=[{\"id\": vector[\"id\"], \"values\": vector[\"values\"], \"metadata\": vector[\"metadata\"]} for vector in vectors], namespace=\"\")\n",
    "#             print(f\"Uploaded Document - Filename: {filename}, embedded_ID: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "\n",
    "# # Main upload function using threading\n",
    "# def upload_documents(directory, embeddings, index, tenant_ID):\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         futures = []\n",
    "#         for filename in os.listdir(directory):\n",
    "#             if filename.endswith(('.txt', '.pdf', '.docx')):  # Process only supported files\n",
    "#                 futures.append(executor.submit(process_document, filename, directory, embeddings, index, tenant_ID))\n",
    "\n",
    "#         # Wait for all futures to complete\n",
    "#         for future in as_completed(futures):\n",
    "#             future.result()  # This will also raise exceptions if any occurred during processing\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Example call to upload documents from the directory\n",
    "# tenant_ID = \"tenant1234\"  # Example tenant ID\n",
    "# directory = 'data'  # Specify the directory containing the documents\n",
    "# upload_documents(directory, embeddings, index, tenant_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# # Define constants\n",
    "# chunk_size = 1000  # Adjust based on your use case\n",
    "# chunk_overlap = 100  # Adjust based on your use case\n",
    "\n",
    "# # Function to read text from different file types\n",
    "# def read_document(file_path):\n",
    "#     if file_path.endswith('.txt'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     elif file_path.endswith('.pdf'):\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             reader = PdfReader(file)\n",
    "#             return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "#     elif file_path.endswith('.docx'):\n",
    "#         doc = docx.Document(file_path)\n",
    "#         return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "#     else:\n",
    "#         return None  # Unsupported file type\n",
    "\n",
    "# # Function to generate a 14-digit random ID\n",
    "# def generate_14_digit_random_id():\n",
    "#     return str(random.randint(10000000000000, 99999999999999))\n",
    "\n",
    "# # Function to process each document\n",
    "# def process_document(filename, directory, embeddings, index):\n",
    "#     file_path = os.path.join(directory, filename)\n",
    "#     document_text = read_document(file_path)\n",
    "    \n",
    "#     if document_text:\n",
    "#         # Generate a random 14-digit document ID prefix\n",
    "#         doc_id_prefix = generate_14_digit_random_id()\n",
    "\n",
    "#         # Split the document into chunks\n",
    "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "#         chunks = text_splitter.split_text(document_text)\n",
    "\n",
    "#         # Generate embeddings for chunks in batches\n",
    "#         vectors = []\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             chunk_id = f\"{doc_id_prefix}#chunk{i+1}\"\n",
    "#             vectors.append({\"id\": chunk_id, \"content\": chunk})\n",
    "\n",
    "#         # Generate embeddings for all chunks at once\n",
    "#         if vectors:\n",
    "#             content_list = [vector[\"content\"] for vector in vectors]\n",
    "#             embedded_vectors = embeddings.embed_documents(content_list)\n",
    "\n",
    "#             # Prepare the vectors for upsert\n",
    "#             for vector, embedding in zip(vectors, embedded_vectors):\n",
    "#                 vector[\"values\"] = embedding\n",
    "#                 vector[\"metadata\"] = {\n",
    "#                     \"source\": filename,\n",
    "#                     \"page_content\": vector[\"content\"][:100],  # Store first 100 characters as a preview\n",
    "#                 }\n",
    "\n",
    "#             # Upsert the vectors (chunks) into Pinecone with metadata\n",
    "#             index.upsert(vectors=[{\"id\": vector[\"id\"], \"values\": vector[\"values\"], \"metadata\": vector[\"metadata\"]} for vector in vectors], namespace=\"\")\n",
    "#             print(f\"Uploaded Document - Filename: {filename}, embedded_ID: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "\n",
    "# # Main upload function using threading\n",
    "# def upload_documents(directory, embeddings, index):\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         futures = []\n",
    "#         for filename in os.listdir(directory):\n",
    "#             if filename.endswith(('.txt', '.pdf', '.docx')):  # Process only supported files\n",
    "#                 futures.append(executor.submit(process_document, filename, directory, embeddings, index))\n",
    "\n",
    "#         # Wait for all futures to complete\n",
    "#         for future in as_completed(futures):\n",
    "#             future.result()  # This will also raise exceptions if any occurred during processing\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Example call to upload documents from the directory\n",
    "# directory = 'data'  # Specify the directory containing the documents\n",
    "# upload_documents(directory, embeddings, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk_size = 500 \n",
    "#chunk_overlap = 50  \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"test-2\"\n",
    "#vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "vectorstore = PineconeVectorStore.from_documents(split_docs, embeddings, index_name=index_name)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"\"\"From the values of X-ray and experimental den- sities, the percentage of porosity of the samples was calculated using the relation,.formula for percentage of porosity\"\"\")\n",
    "print(format_docs(retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how to make an appointment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in index.list(namespace=\"\"):\n",
    " query = index.query(\n",
    " id=ids[0], \n",
    " top_k=1,\n",
    " include_values=True,\n",
    " include_metadata=True\n",
    " )\n",
    " print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'how to make an appointment ',\n",
       " 'result': 'To make a new appointment, follow these steps:\\n\\n1. Click the +Add button.\\n2. Select the client from the drop-down menu.\\n3. If the client is Checked In at the time, click on Is Checked.\\n4. Proceed to select the appointment date and time.'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "\n",
    ")\n",
    "qa.invoke(\"how to make an appointment \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an expert LLM assistant specialized in answering questions based solely on the information provided in the uploaded documents (PDF, DOCX, or TXT formats). Use only the information from the documents to respond accurately and clearly to each question.\n",
    "\n",
    "Guidelines:\n",
    "1. Avoid using outside knowledge or assumptions. Stick strictly to the content in the documents.\n",
    "2. If the answer is not found in the uploaded documents, state, \"The answer is not specifically mentioned in the provided documents.\"\n",
    "3. Avoid using outside knowledge or assumptions. Stick strictly to the content in the documents.\n",
    "4. Maintain a professional and helpful tone thinking you are giving service to the customer for their documents.\n",
    "5. Answer for normal conversation questions like \"Hi\", \"Hey\", \"Hello\", \"How are you?\", and many others with the answer: \"Hello, How can I assist you?\".\n",
    "6. If the question is on \"summarize\" or \"summarization\", then summarize the documents to (1/4)th the size of the original documents.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"how to make an appointment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With and Without Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n",
      "\n",
      "Chat with knowledge:\n",
      "To make a new appointment, follow these steps:\n",
      "\n",
      "1. Click the +Add button to start adding a new appointment.\n",
      "2. Select the client from the drop-down menu.\n",
      "3. If the client is checked in at the time, click on Is Checked.\n",
      "4. Proceed to select the appointment date and time.\n",
      "\n",
      "Chat without knowledge:\n",
      "To make an appointment, follow these general steps:\n",
      "\n",
      "1. **Identify the Purpose**: Determine the reason for the appointment, whether it's a medical visit, business meeting, or personal engagement.\n",
      "\n",
      "2. **Choose the Service Provider**: Decide where or with whom you need to make the appointment. This could be a doctor's office, a business, or an individual.\n",
      "\n",
      "3. **Contact Information**: Find the contact details of the service provider. This could be a phone number, email address, or an online booking system.\n",
      "\n",
      "4. **Check Availability**: Look for available dates and times. This might involve checking an online calendar or calling to ask about open slots.\n",
      "\n",
      "5. **Make the Appointment**:\n",
      "   - **Phone**: Call the service provider and speak to a receptionist or scheduler. Clearly state your name, the purpose of the appointment, and your preferred date and time.\n",
      "   - **Online**: Use the provider's website or app to select a date and time. Fill in any required information and confirm the booking.\n",
      "   - **Email**: Send an email requesting an appointment, including your preferred dates and times, and wait for a confirmation.\n",
      "\n",
      "6. **Confirm Details**: Once the appointment is scheduled, confirm the date, time, location, and any other necessary details. Make sure you understand any preparation needed before the appointment.\n",
      "\n",
      "7. **Set a Reminder**: Use a calendar app or a physical planner to note the appointment and set a reminder to ensure you dont forget.\n",
      "\n",
      "8. **Follow Up**: If necessary, follow up with the service provider to confirm the appointment closer to the date.\n",
      "\n",
      "These steps can vary slightly depending on the type of appointment and the service provider's procedures.\n"
     ]
    }
   ],
   "source": [
    "# Send each query to the LLM twice, first with relevant knowledge from Pincone \n",
    "# and then without any additional knowledge.\n",
    "print(\"Query 1\\n\")\n",
    "print(\"Chat with knowledge:\")\n",
    "print(qa.invoke(query1).get(\"result\"))\n",
    "print(\"\\nChat without knowledge:\")\n",
    "print(llm.invoke(query1).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_rag_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# # Create the parallel chain\n",
    "# My_rag_chain = RunnableParallel(\n",
    "#     {\n",
    "#         \"context\": retriever | format_docs,\n",
    "#         \"question\": RunnablePassthrough()\n",
    "#     }\n",
    "# ) | custom_rag_template | llm | StrOutputParser()\n",
    "\n",
    "# ## My chain : Retriever(Pinecone) | custom_rag_template(prompt) | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword = \"How to create a client?\"\n",
    "# template = \"\"\"You are an expert LLM assistant specialized in answering questions based solely on the information provided in the uploaded documents (PDF, DOCX, or TXT formats). Use only the information from the documents to respond accurately and clearly to each question.\n",
    "\n",
    "# Guidelines:\n",
    "# 1. Avoid using outside knowledge or assumptions. Stick strictly to the content in the documents.\n",
    "# 2. If the answer is not found in the uploaded documents, state, \"The answer is not specifically mentioned in the provided documents.\"\n",
    "# 3. Avoid using outside knowledge or assumptions. Stick strictly to the content in the documents.\n",
    "# 4. Maintain a professional and helpful tone thinking you are giving service to the customer for their documents.\n",
    "# 5. Answer for normal conversation questions like \"Hi\", \"Hey\", \"Hello\", \"How are you?\", and many others with the answer: \"Hello, How can I assist you?\".\n",
    "# 6. If the question is on \"summarize\" or \"summarization\", then summarize the documents to (1/4)th the size of the original documents.\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Context: {context}\n",
    "\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = template.format(question=keyword, context=format_docs(retrieved_docs))\n",
    "\n",
    "# # Create the LLM\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-4o\",\n",
    "#     temperature=0.1,\n",
    "# )\n",
    "\n",
    "# # Set up the retrieval QA with custom response for missing answers\n",
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     retriever=vectorstore.as_retriever(),\n",
    "#     return_source_documents=True  # Ensures that source documents are returned\n",
    "# )\n",
    "\n",
    "# # Wrapper function to handle missing answers\n",
    "# def get_answer(keyword):\n",
    "#     response = qa.invoke(keyword)\n",
    "    \n",
    "#     # Check if the response contains valid data\n",
    "#     result = response.get('result', '')\n",
    "#     if \"The answer is not specifically mentioned\" in result or not result.strip():\n",
    "#         return \"Precise answer not found in documents, try another prompt.\"\n",
    "#     else:\n",
    "#         return result\n",
    "\n",
    "# # Test the response with the keyword quer\n",
    "# final_answer = get_answer(\"how to make a membership?\")\n",
    "# print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"test-2\",\n",
       "    \"dimension\": 1536,\n",
       "    \"metric\": \"dotproduct\",\n",
       "    \"host\": \"test-2-5vwf04k.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"deletion_protection\": \"disabled\"\n",
       "}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.describe_index(\"test-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to delete all chunks of a document by prefix\n",
    "# def delete_document_by_prefix(doc_id_prefix):\n",
    "#     chunk_ids = list_document_chunks(doc_id_prefix)\n",
    "#     if chunk_ids:\n",
    "#         index.delete(ids=chunk_ids, namespace=\"\")\n",
    "#         print(f\"Deleted {len(chunk_ids)} chunks for document: {doc_id_prefix}\")\n",
    "#     else:\n",
    "#         print(f\"No chunks found for document: {doc_id_prefix}\")\n",
    "\n",
    "# # Example usage\n",
    "# # Upload documents from the directory\n",
    "# upload_documents(directory, embeddings, index)\n",
    "\n",
    "# # List all chunk IDs for a document\n",
    "# doc_id_prefix = \"cricket_rules\"  # Replace with your actual document name\n",
    "# chunks = list_document_chunks(doc_id_prefix)\n",
    "# print(f\"Chunks for document '{doc_id_prefix}': {chunks}\")\n",
    "\n",
    "# # Delete all chunks for a document\n",
    "# delete_document_by_prefix(doc_id_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index.delete(delete_all=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
