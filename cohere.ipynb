{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \\\n",
    "#   langchain_community \\\n",
    "#   langchain_pinecone \\\n",
    "#   langchain_openai \\\n",
    "#   unstructured \\\n",
    "#   langchain-text-splitters \\\n",
    "#   pinecone-text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import (\n",
    "    PineconeHybridSearchRetriever)\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_core.runnables import Runnable\n",
    "from pinecone import ServerlessSpec\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pinecone\n",
    "import uuid\n",
    "import os\n",
    "import glob\n",
    "import hashlib\n",
    "from tqdm.autonotebook import tqdm\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import nltk\n",
    "#  nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Fetch API keys from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "\n",
    "# Set the environment variables\n",
    "if openai_api_key:\n",
    "    os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ['PINECONE_API_KEY'] = pinecone_api_key \n",
    "\n",
    "#Verify that the keys are loaded\n",
    "#print(f\"OpenAI API Key: {os.environ.get('OPENAI_API_KEY')}\")\n",
    "#print(f\"Pinecone API Key: {os.environ.get('PINECONE_API_KEY')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data'\n",
    "\n",
    "def load_docs(directory):\n",
    "    loader = DirectoryLoader(directory)\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "docs = load_docs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs[0].page_content[0:100])\n",
    "# print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x31f1b31d0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x323d7f5c0>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"HUGGINGFACEHUB_API_TOKEN\"\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",  #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    ")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import time\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "import time\n",
    "\n",
    "index_name = \"test-2\"  # change if desired\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "#pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader  # Assuming DirectoryLoader is part of LangChain\n",
    "\n",
    "# directory = 'data'\n",
    "\n",
    "# # Function to load documents from the directory\n",
    "# def load_docs(directory):\n",
    "#     loader = DirectoryLoader(directory)\n",
    "#     docs = loader.load()  # Load documents using the DirectoryLoader\n",
    "#     return docs\n",
    "\n",
    "# # Upload documents and store metadata including source and text\n",
    "# def upload_documents(docs, embeddings, index):\n",
    "#     for doc in docs:\n",
    "#         # Extract document content and metadata\n",
    "#         document_text = doc.page_content\n",
    "#         source = doc.metadata.get('source', 'unknown')  # Assuming 'source' is part of doc.metadata\n",
    "\n",
    "#         if document_text:  # Only proceed if the document was read successfully\n",
    "#             # 1. Generate embedding for the document\n",
    "#             vector = embeddings.embed_documents([document_text])[0]  # Use embed_documents for document embeddings\n",
    "\n",
    "#             # 2. Generate a unique vector ID (UUID)\n",
    "#             vector_id = str(uuid.uuid4())  # Generate a unique UUID\n",
    "            \n",
    "#             # 3. Prepare metadata (source and content preview)\n",
    "#             metadata = {\n",
    "#                 \"source\": source,  # The document name as source\n",
    "#                 \"text\": document_text[:100]  # Store first 100 characters as a preview of the document\n",
    "#             }\n",
    "            \n",
    "#             # 4. Upsert the vector into Pinecone with metadata\n",
    "#             index.upsert(vectors=[(vector_id, vector, metadata)])  # Make sure to include metadata here\n",
    "\n",
    "#             # 5. Print vector ID and document name\n",
    "#             print(f\"Uploaded Document - Vector ID: {vector_id}, Document Source: {source}\")\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Load documents using load_docs function\n",
    "# docs = load_docs(directory)\n",
    "\n",
    "# # Call the function to upload documents into Pinecone vector store\n",
    "# upload_documents(docs, embeddings, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader  # Assuming DirectoryLoader is part of LangChain\n",
    "\n",
    "# directory = 'data'\n",
    "\n",
    "# # Function to load documents from the directory\n",
    "# def load_docs(directory):\n",
    "#     loader = DirectoryLoader(directory)\n",
    "#     docs = loader.load()  # Load documents using the DirectoryLoader\n",
    "#     return docs\n",
    "\n",
    "# # Upload documents and store metadata including source and text\n",
    "# def upload_documents(docs, embeddings, index, tenant_id):\n",
    "#     for doc in docs:\n",
    "#         # Extract document content and metadata\n",
    "#         document_text = doc.page_content\n",
    "#         source = doc.metadata.get('source', 'unknown')  # Assuming 'source' is part of doc.metadata\n",
    "\n",
    "#         if document_text:  # Only proceed if the document was read successfully\n",
    "#             # 1. Generate embedding for the document\n",
    "#             vector = embeddings.embed_documents([document_text])[0]  # Use embed_documents for document embeddings\n",
    "\n",
    "#             # 2. Generate a unique vector ID (UUID)\n",
    "#             vector_id = str(uuid.uuid4())  # Generate a unique UUID\n",
    "            \n",
    "#             # 3. Prepare metadata (source, tenant_id, and content preview)\n",
    "#             metadata = {\n",
    "#                 \"source\": source,  # The document name as source\n",
    "#                 \"tenant_id\": tenant_id,  # Add tenant-specific ID\n",
    "#                 \"text\": document_text[:100]  # Store first 100 characters as a preview of the document\n",
    "#             }\n",
    "            \n",
    "#             # 4. Upsert the vector into Pinecone with metadata\n",
    "#             index.upsert(vectors=[{\"id\": vector_id, \"values\": vector, \"metadata\": metadata}])  # Include metadata here\n",
    "\n",
    "#             # 5. Print vector ID and document name\n",
    "#             print(f\"Uploaded Document - Vector ID: {vector_id}, Document Source: {source}, Tenant ID: {tenant_id}\")\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Load documents using load_docs function\n",
    "# docs = load_docs(directory)\n",
    "\n",
    "# # Example of embedding generation and Pinecone index setup\n",
    "# # embeddings and index should already be initialized\n",
    "# tenant_id = \"example_tenant_id\"  # Add your tenant_id here\n",
    "\n",
    "# # Call the function to upload documents into Pinecone vector store\n",
    "# upload_documents(docs, embeddings, index, tenant_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader  # Assuming DirectoryLoader is part of LangChain\n",
    "\n",
    "# directory = 'data'\n",
    "\n",
    "# # Function to load documents from the directory\n",
    "# def load_docs(directory):\n",
    "#     loader = DirectoryLoader(directory)\n",
    "#     docs = loader.load()  # Load documents using the DirectoryLoader\n",
    "#     return docs\n",
    "\n",
    "# # Upload documents and store metadata including source and text\n",
    "# def upload_documents(docs, embeddings, index):\n",
    "#     for doc in docs:\n",
    "#         # Extract document content and metadata\n",
    "#         document_text = doc.page_content\n",
    "#         source = doc.metadata.get('source', 'unknown')  # Assuming 'source' is part of doc.metadata\n",
    "\n",
    "#         if document_text:  # Only proceed if the document was read successfully\n",
    "#             # 1. Generate embedding for the document\n",
    "#             vector = embeddings.embed_documents([document_text])[0]  # Use embed_documents for document embeddings\n",
    "\n",
    "#             # 2. Generate a unique vector ID (UUID)\n",
    "#             vector_id = str(uuid.uuid4())  # Generate a unique UUID\n",
    "            \n",
    "#             # 3. Prepare metadata (source and content preview)\n",
    "#             metadata = {\n",
    "#                 \"source\": source,  # The document name as source\n",
    "#                 \"text\": document_text[:100]  # Store first 100 characters as a preview of the document\n",
    "#             }\n",
    "            \n",
    "#             # 4. Upsert the vector into Pinecone with metadata\n",
    "#             index.upsert(vectors=[{\"id\": vector_id, \"values\": vector, \"metadata\": metadata}])  # Include metadata here\n",
    "\n",
    "#             # 5. Print vector ID and document name\n",
    "#             print(f\"Uploaded Document - Vector ID: {vector_id}, Document Source: {source}\")\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Load documents using load_docs function\n",
    "# docs = load_docs(directory)\n",
    "\n",
    "# # Example of embedding generation and Pinecone index setup\n",
    "# # embeddings and index should already be initialized\n",
    "\n",
    "# # Call the function to upload documents into Pinecone vector store\n",
    "# upload_documents(docs, embeddings, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "# directory = 'data'\n",
    "# def load_docs(directory):\n",
    "#     loader = DirectoryLoader(directory)\n",
    "#     docs = loader.load()  # Load documents using the DirectoryLoader\n",
    "#     return docs\n",
    "\n",
    "# chunk_size = 1000  # Adjust based on your use case\n",
    "# chunk_overlap = 100  # Adjust based on your use case\n",
    "\n",
    "# # Function to read text from different file types\n",
    "# def read_document(file_path):\n",
    "#     if file_path.endswith('.txt'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     elif file_path.endswith('.pdf'):\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             reader = PdfReader(file)\n",
    "#             return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "#     elif file_path.endswith('.docx'):\n",
    "#         doc = docx.Document(file_path)\n",
    "#         return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "#     else:\n",
    "#         return None  # Unsupported file type\n",
    "\n",
    "# # Function to upload document chunks with ID prefixes\n",
    "# def upload_documents(directory, embeddings, index):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "#     # Loop through files in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         document_text = read_document(file_path)\n",
    "        \n",
    "#         if document_text:\n",
    "#             # Generate the document ID prefix\n",
    "#             doc_id_prefix = f\"{filename.split('.')[0]}\"\n",
    "\n",
    "#             # Split the document into chunks\n",
    "#             chunks = text_splitter.split_text(document_text)\n",
    "            \n",
    "#             vectors = []\n",
    "#             for i, chunk in enumerate(chunks):\n",
    "#                 # Create unique ID for each chunk\n",
    "#                 chunk_id = f\"{doc_id_prefix}#chunk{i+1}\"\n",
    "#                 # Generate embedding for each chunk\n",
    "#                 vector = embeddings.embed_documents([chunk])[0]\n",
    "#                 # Prepare metadata (source and content preview)\n",
    "#                 metadata = {\n",
    "#                     \"source\": filename,\n",
    "#                     \"page_content\": chunk[:100],  # Store first 100 characters as a preview of the chunk\n",
    "#                 }\n",
    "#                 # Append vector with ID and metadata\n",
    "#                 vectors.append({\"id\": chunk_id, \"values\": vector, \"metadata\": metadata})\n",
    "\n",
    "#             # Upsert the vectors (chunks) into Pinecone with metadata\n",
    "#             index.upsert(vectors=vectors, namespace=\"\")\n",
    "\n",
    "#             print(f\"Uploaded Document - Prefix: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Call the function to upload documents from the directory\n",
    "# upload_documents(directory, embeddings, index)\n",
    "\n",
    "# # Function to list all chunks of a document by prefix\n",
    "# def list_document_chunks(doc_id_prefix):\n",
    "#     chunk_ids = []\n",
    "#     for ids in index.list(prefix=f\"{doc_id_prefix}#\", namespace=\"\"):\n",
    "#         chunk_ids.extend(ids)\n",
    "#     return chunk_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# import random\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# directory = 'data'\n",
    "# def load_docs(directory):\n",
    "#     loader = DirectoryLoader(directory)\n",
    "#     docs = loader.load()  # Load documents using the DirectoryLoader\n",
    "#     return docs\n",
    "\n",
    "# chunk_size = 1000  # Adjust based on your use case\n",
    "# chunk_overlap = 100  # Adjust based on your use case\n",
    "\n",
    "# # Function to read text from different file types\n",
    "# def read_document(file_path):\n",
    "#     if file_path.endswith('.txt'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     elif file_path.endswith('.pdf'):\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             reader = PdfReader(file)\n",
    "#             return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "#     elif file_path.endswith('.docx'):\n",
    "#         doc = docx.Document(file_path)\n",
    "#         return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "#     else:\n",
    "#         return None  # Unsupported file type\n",
    "\n",
    "# # Function to generate a random 8-digit ID\n",
    "# def generate_random_id():\n",
    "#     return str(random.randint(10000000, 99999999))\n",
    "\n",
    "# # Function to upload document chunks with a random 8-digit ID prefix\n",
    "# def upload_documents(directory, embeddings, index):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "#     # Loop through files in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         document_text = read_document(file_path)\n",
    "        \n",
    "#         if document_text:\n",
    "#             # Generate a random 8-digit document ID prefix\n",
    "#             doc_id_prefix = generate_random_id()\n",
    "\n",
    "#             # Split the document into chunks\n",
    "#             chunks = text_splitter.split_text(document_text)\n",
    "            \n",
    "#             vectors = []\n",
    "#             for i, chunk in enumerate(chunks):\n",
    "#                 # Create unique ID for each chunk\n",
    "#                 chunk_id = f\"{doc_id_prefix}#chunk{i+1}\"\n",
    "#                 # Generate embedding for each chunk\n",
    "#                 vector = embeddings.embed_documents([chunk])[0]\n",
    "#                 # Prepare metadata (source and content preview)\n",
    "#                 metadata = {\n",
    "#                     \"source\": filename,\n",
    "#                     \"page_content\": chunk[:100],  # Store first 100 characters as a preview of the chunk\n",
    "#                 }\n",
    "#                 # Append vector with ID and metadata\n",
    "#                 vectors.append({\"id\": chunk_id, \"values\": vector, \"metadata\": metadata})\n",
    "\n",
    "#             # Upsert the vectors (chunks) into Pinecone with metadata\n",
    "#             index.upsert(vectors=vectors, namespace=\"\")\n",
    "\n",
    "#             print(f\"Uploaded Document - Prefix: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Function to list all chunks of a document by prefix\n",
    "# def list_document_chunks(doc_id_prefix):\n",
    "#     chunk_ids = []\n",
    "#     for ids in index.list(prefix=f\"{doc_id_prefix}#\", namespace=\"\"):\n",
    "#         chunk_ids.extend(ids)\n",
    "#     return chunk_ids\n",
    "\n",
    "# # Call the function to upload documents from the directory\n",
    "# upload_documents(directory, embeddings, index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping unsupported or empty file: data/.DS_Store\n",
      "Uploaded Document - Prefix: Activity Logs Module , Total Chunks: 1\n",
      "Uploaded Document - Prefix: Clients - Comms Module , Total Chunks: 1\n",
      "Uploaded Document - Prefix: Virtual Visits Module , Total Chunks: 2\n",
      "Uploaded Document - Prefix: Clients - Cases Module , Total Chunks: 4\n",
      "Uploaded Document - Prefix: Files Module , Total Chunks: 2\n",
      "Uploaded Document - Prefix: Comms Module , Total Chunks: 2\n",
      "Uploaded Document - Prefix: Clients - Billings Module , Total Chunks: 2\n",
      "Uploaded Document - Prefix: Leads Module , Total Chunks: 4\n",
      "Uploaded Document - Prefix: Clients - Clients Module , Total Chunks: 5\n",
      "Uploaded Document - Prefix: Reports Module , Total Chunks: 2\n",
      "Uploaded Document - Prefix:  Billings Module , Total Chunks: 6\n",
      "Uploaded Document - Prefix: Cases Module , Total Chunks: 8\n",
      "Uploaded Document - Prefix: Calendar Module , Total Chunks: 4\n",
      "Uploaded Document - Prefix: Appointments Module , Total Chunks: 3\n",
      "Uploaded Document - Prefix: Settings Module, Total Chunks: 7\n",
      "Uploaded Document - Prefix: Clients Module , Total Chunks: 16\n",
      "All documents uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "from PyPDF2 import PdfReader  # For reading PDF files\n",
    "import docx\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "directory = 'data'\n",
    "chunk_size = 1000  # Adjust based on your use case\n",
    "chunk_overlap = 100  # Adjust based on your use case\n",
    "batch_size = 50  # Number of chunks per batch for upsert to Pinecone\n",
    "\n",
    "\n",
    "# Function to read text from different file types\n",
    "def read_document(file_path):\n",
    "    if file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PdfReader(file)\n",
    "            return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "    elif file_path.endswith('.docx'):\n",
    "        doc = docx.Document(file_path)\n",
    "        return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "    else:\n",
    "        return None  # Unsupported file type\n",
    "\n",
    "\n",
    "# Function to process and upsert a single document\n",
    "def process_and_upload_document(file_path, embeddings, index, tenant_ID):\n",
    "    document_text = read_document(file_path)\n",
    "\n",
    "    if document_text:\n",
    "        # Generate the document ID prefix\n",
    "        doc_id_prefix = f\"{os.path.basename(file_path).split('.')[0]}\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "        # Split the document into chunks\n",
    "        chunks = text_splitter.split_text(document_text)\n",
    "        vectors = []\n",
    "        \n",
    "        # Create vectors for each chunk\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_id = f\"{doc_id_prefix}#chunk{i+1}\"\n",
    "            vector = embeddings.embed_documents([chunk])[0]\n",
    "            metadata = {\n",
    "                \"filename\": os.path.basename(file_path),\n",
    "                \"tenant_ID\": tenant_ID,  # Add tenant_ID to metadata\n",
    "            }\n",
    "            vectors.append({\"id\": chunk_id, \"values\": vector, \"metadata\": metadata})\n",
    "\n",
    "            # Batch upsert after processing `batch_size` chunks\n",
    "            if len(vectors) >= batch_size:\n",
    "                index.upsert(vectors=vectors, namespace=\"\")\n",
    "                vectors.clear()  # Clear the batch after uploading\n",
    "\n",
    "        # Upsert any remaining vectors\n",
    "        if vectors:\n",
    "            index.upsert(vectors=vectors, namespace=\"\")\n",
    "        \n",
    "        print(f\"Uploaded Document - Prefix: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "    else:\n",
    "        print(f\"Skipping unsupported or empty file: {file_path}\")\n",
    "\n",
    "\n",
    "# Function to upload documents in parallel\n",
    "def upload_documents_concurrently(directory, embeddings, index, tenant_ID):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            futures.append(executor.submit(process_and_upload_document, file_path, embeddings, index, tenant_ID))\n",
    "        \n",
    "        # Wait for all threads to complete\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "    print(\"All documents uploaded successfully!\")\n",
    "\n",
    "\n",
    "# Call the function to upload documents from the directory concurrently\n",
    "tenant_ID = \"your_tenant_ID_here\"  # Add tenant ID here or pass dynamically\n",
    "upload_documents_concurrently(directory, embeddings, index, tenant_ID)\n",
    "\n",
    "\n",
    "# Function to list all chunks of a document by prefix\n",
    "def list_document_chunks(doc_id_prefix):\n",
    "    chunk_ids = []\n",
    "    for ids in index.list(prefix=f\"{doc_id_prefix}#\", namespace=\"\"):\n",
    "        chunk_ids.extend(ids)\n",
    "    return chunk_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 7 chunks for document: Settings Module\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "# Function to list all chunks of a document by prefix\n",
    "def list_document_chunks(doc_id_prefix):\n",
    "    chunk_ids = []\n",
    "    # Assuming index.list supports pagination or has a mechanism to retrieve chunks efficiently\n",
    "    for ids in index.list(prefix=f\"{doc_id_prefix}#\", namespace=\"\"):\n",
    "        chunk_ids.extend(ids)\n",
    "    return chunk_ids\n",
    "\n",
    "\n",
    "# Function to delete all chunks of a document by prefix in parallel\n",
    "def delete_document_by_prefix(doc_id_prefix, batch_size=100):\n",
    "    chunk_ids = list_document_chunks(doc_id_prefix)\n",
    "    \n",
    "    if chunk_ids:\n",
    "        def delete_chunks_batch(batch):\n",
    "            index.delete(ids=batch, namespace=\"\")\n",
    "        \n",
    "        # Batch the chunk deletions to reduce overhead\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Split chunk_ids into batches of size `batch_size`\n",
    "            batches = [chunk_ids[i:i + batch_size] for i in range(0, len(chunk_ids), batch_size)]\n",
    "            futures = [executor.submit(delete_chunks_batch, batch) for batch in batches]\n",
    "\n",
    "            # Wait for all batch deletions to complete\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                future.result()\n",
    "\n",
    "        print(f\"Deleted {len(chunk_ids)} chunks for document: {doc_id_prefix}\")\n",
    "    else:\n",
    "        print(f\"No chunks found for document: {doc_id_prefix}\")\n",
    "\n",
    "\n",
    "# Function to upload documents in parallel (unchanged)\n",
    "def upload_documents_concurrently(directory, embeddings, index, tenant_ID):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            futures.append(executor.submit(process_and_upload_document, file_path, embeddings, index, tenant_ID))\n",
    "        \n",
    "        # Wait for all threads to complete\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "    print(\"All documents uploaded successfully!\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "doc_id_prefix = \"Settings Module\"  # Replace with your actual document name\n",
    "\n",
    "# Delete all chunks for a document in parallel\n",
    "delete_document_by_prefix(doc_id_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uuid\n",
    "# import random\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # directory = 'data'\n",
    "# chunk_size = 1000  # Adjust based on your use case\n",
    "# chunk_overlap = 100  # Adjust based on your use case\n",
    "\n",
    "# # # Function to load documents\n",
    "# # def load_docs(directory):\n",
    "# #     loader = DirectoryLoader(directory)\n",
    "# #     docs = loader.load()  # Load documents using the DirectoryLoader\n",
    "# #     return docs\n",
    "\n",
    "# # Function to read text from different file types\n",
    "# def read_document(file_path):\n",
    "#     if file_path.endswith('.txt'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     elif file_path.endswith('.pdf'):\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             reader = PdfReader(file)\n",
    "#             return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "#     elif file_path.endswith('.docx'):\n",
    "#         doc = docx.Document(file_path)\n",
    "#         return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "#     else:\n",
    "#         return None  # Unsupported file type\n",
    "\n",
    "# # Function to generate a 14-digit random ID\n",
    "# def generate_14_digit_random_id():\n",
    "#     return str(random.randint(10000000000000, 99999999999999))\n",
    "\n",
    "# # Function to upload document chunks with a random 14-digit ID prefix\n",
    "# def upload_documents(directory, embeddings, index, tenant_ID):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "#     # Loop through files in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         document_text = read_document(file_path)\n",
    "        \n",
    "#         if document_text:\n",
    "#             # Generate a random 14-digit document ID prefix\n",
    "#             doc_id_prefix = generate_14_digit_random_id()\n",
    "\n",
    "#             # Split the document into chunks\n",
    "#             chunks = text_splitter.split_text(document_text)\n",
    "            \n",
    "#             vectors = []\n",
    "#             for i, chunk in enumerate(chunks):\n",
    "#                 # Create unique ID for each chunk\n",
    "#                 chunk_id = f\"{doc_id_prefix}#chunk{i+1}\"\n",
    "#                 # Generate embedding for each chunk\n",
    "#                 vector = embeddings.embed_documents([chunk])[0]\n",
    "#                 # Prepare metadata (source, tenant_ID, and content preview)\n",
    "#                 metadata = {\n",
    "#                     \"source\": filename,\n",
    "#                     \"tenant_ID\": tenant_ID,  # Store the tenant ID as metadata\n",
    "#                     \"page_content\": chunk[:100],  # Store first 100 characters as a preview of the chunk\n",
    "#                 }\n",
    "#                 # Append vector with ID and metadata\n",
    "#                 vectors.append({\"id\": chunk_id, \"values\": vector, \"metadata\": metadata})\n",
    "\n",
    "#             # Upsert the vectors (chunks) into Pinecone with metadata\n",
    "#             index.upsert(vectors=vectors, namespace=\"\")\n",
    "\n",
    "#             #print(f\"Uploaded Document - Prefix: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "#             print(f\"Uploaded Document - Filename: {filename}, embedded_ID: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Function to list all chunks of a document by prefix\n",
    "# def list_document_chunks(doc_id_prefix):\n",
    "#     chunk_ids = []\n",
    "#     for ids in index.list(prefix=f\"{doc_id_prefix}#\", namespace=\"\"):\n",
    "#         chunk_ids.extend(ids)\n",
    "#     return chunk_ids\n",
    "\n",
    "# # Example call to upload documents from the directory\n",
    "# tenant_ID = \"tenant1234\"  # Example tenant ID\n",
    "# upload_documents(directory, embeddings, index, tenant_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# # Define constants\n",
    "# chunk_size = 1000  # Adjust based on your use case\n",
    "# chunk_overlap = 100  # Adjust based on your use case\n",
    "\n",
    "# # Function to read text from different file types\n",
    "# def read_document(file_path):\n",
    "#     if file_path.endswith('.txt'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     elif file_path.endswith('.pdf'):\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             reader = PdfReader(file)\n",
    "#             return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "#     elif file_path.endswith('.docx'):\n",
    "#         doc = docx.Document(file_path)\n",
    "#         return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "#     else:\n",
    "#         return None  # Unsupported file type\n",
    "\n",
    "# # Function to generate a 14-digit random ID\n",
    "# def generate_14_digit_random_id():\n",
    "#     return str(random.randint(10000000000000, 99999999999999))\n",
    "\n",
    "# # Function to process each document\n",
    "# def process_document(filename, directory, embeddings, index, tenant_ID):\n",
    "#     file_path = os.path.join(directory, filename)\n",
    "#     document_text = read_document(file_path)\n",
    "    \n",
    "#     if document_text:\n",
    "#         # Generate a random 14-digit document ID prefix\n",
    "#         doc_id_prefix = generate_14_digit_random_id()\n",
    "\n",
    "#         # Split the document into chunks\n",
    "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "#         chunks = text_splitter.split_text(document_text)\n",
    "\n",
    "#         # Generate embeddings for chunks in batches\n",
    "#         vectors = []\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             chunk_id = f\"{doc_id_prefix}#chunk{i+1}\"\n",
    "#             vectors.append({\"id\": chunk_id, \"content\": chunk})\n",
    "\n",
    "#         # Generate embeddings for all chunks at once\n",
    "#         if vectors:\n",
    "#             content_list = [vector[\"content\"] for vector in vectors]\n",
    "#             embedded_vectors = embeddings.embed_documents(content_list)\n",
    "\n",
    "#             # Prepare the vectors for upsert\n",
    "#             for vector, embedding in zip(vectors, embedded_vectors):\n",
    "#                 vector[\"values\"] = embedding\n",
    "#                 vector[\"metadata\"] = {\n",
    "#                     \"source\": filename,\n",
    "#                     \"tenant_ID\": tenant_ID,\n",
    "#                     \"page_content\": vector[\"content\"][:100],  # Store first 100 characters as a preview\n",
    "#                 }\n",
    "\n",
    "#             # Upsert the vectors (chunks) into Pinecone with metadata\n",
    "#             index.upsert(vectors=[{\"id\": vector[\"id\"], \"values\": vector[\"values\"], \"metadata\": vector[\"metadata\"]} for vector in vectors], namespace=\"\")\n",
    "#             print(f\"Uploaded Document - Filename: {filename}, embedded_ID: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "\n",
    "# # Main upload function using threading\n",
    "# def upload_documents(directory, embeddings, index, tenant_ID):\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         futures = []\n",
    "#         for filename in os.listdir(directory):\n",
    "#             if filename.endswith(('.txt', '.pdf', '.docx')):  # Process only supported files\n",
    "#                 futures.append(executor.submit(process_document, filename, directory, embeddings, index, tenant_ID))\n",
    "\n",
    "#         # Wait for all futures to complete\n",
    "#         for future in as_completed(futures):\n",
    "#             future.result()  # This will also raise exceptions if any occurred during processing\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Example call to upload documents from the directory\n",
    "# tenant_ID = \"tenant1234\"  # Example tenant ID\n",
    "# directory = 'data'  # Specify the directory containing the documents\n",
    "# upload_documents(directory, embeddings, index, tenant_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# from PyPDF2 import PdfReader  # For reading PDF files\n",
    "# import docx\n",
    "# from langchain.document_loaders import DirectoryLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# # Define constants\n",
    "# chunk_size = 1000  # Adjust based on your use case\n",
    "# chunk_overlap = 100  # Adjust based on your use case\n",
    "\n",
    "# # Function to read text from different file types\n",
    "# def read_document(file_path):\n",
    "#     if file_path.endswith('.txt'):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     elif file_path.endswith('.pdf'):\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             reader = PdfReader(file)\n",
    "#             return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "#     elif file_path.endswith('.docx'):\n",
    "#         doc = docx.Document(file_path)\n",
    "#         return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "#     else:\n",
    "#         return None  # Unsupported file type\n",
    "\n",
    "# # Function to generate a 14-digit random ID\n",
    "# def generate_14_digit_random_id():\n",
    "#     return str(random.randint(10000000000000, 99999999999999))\n",
    "\n",
    "# # Function to process each document\n",
    "# def process_document(filename, directory, embeddings, index):\n",
    "#     file_path = os.path.join(directory, filename)\n",
    "#     document_text = read_document(file_path)\n",
    "    \n",
    "#     if document_text:\n",
    "#         # Generate a random 14-digit document ID prefix\n",
    "#         doc_id_prefix = generate_14_digit_random_id()\n",
    "\n",
    "#         # Split the document into chunks\n",
    "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "#         chunks = text_splitter.split_text(document_text)\n",
    "\n",
    "#         # Generate embeddings for chunks in batches\n",
    "#         vectors = []\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             chunk_id = f\"{doc_id_prefix}#chunk{i+1}\"\n",
    "#             vectors.append({\"id\": chunk_id, \"content\": chunk})\n",
    "\n",
    "#         # Generate embeddings for all chunks at once\n",
    "#         if vectors:\n",
    "#             content_list = [vector[\"content\"] for vector in vectors]\n",
    "#             embedded_vectors = embeddings.embed_documents(content_list)\n",
    "\n",
    "#             # Prepare the vectors for upsert\n",
    "#             for vector, embedding in zip(vectors, embedded_vectors):\n",
    "#                 vector[\"values\"] = embedding\n",
    "#                 vector[\"metadata\"] = {\n",
    "#                     \"source\": filename,\n",
    "#                     \"page_content\": vector[\"content\"][:100],  # Store first 100 characters as a preview\n",
    "#                 }\n",
    "\n",
    "#             # Upsert the vectors (chunks) into Pinecone with metadata\n",
    "#             index.upsert(vectors=[{\"id\": vector[\"id\"], \"values\": vector[\"values\"], \"metadata\": vector[\"metadata\"]} for vector in vectors], namespace=\"\")\n",
    "#             print(f\"Uploaded Document - Filename: {filename}, embedded_ID: {doc_id_prefix}, Total Chunks: {len(chunks)}\")\n",
    "\n",
    "# # Main upload function using threading\n",
    "# def upload_documents(directory, embeddings, index):\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         futures = []\n",
    "#         for filename in os.listdir(directory):\n",
    "#             if filename.endswith(('.txt', '.pdf', '.docx')):  # Process only supported files\n",
    "#                 futures.append(executor.submit(process_document, filename, directory, embeddings, index))\n",
    "\n",
    "#         # Wait for all futures to complete\n",
    "#         for future in as_completed(futures):\n",
    "#             future.result()  # This will also raise exceptions if any occurred during processing\n",
    "\n",
    "#     print(\"All documents uploaded successfully!\")\n",
    "\n",
    "# # Example call to upload documents from the directory\n",
    "# directory = 'data'  # Specify the directory containing the documents\n",
    "# upload_documents(directory, embeddings, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk_size = 500 \n",
    "#chunk_overlap = 50  \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"test-2\"\n",
    "#vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "vectorstore = PineconeVectorStore.from_documents(split_docs, embeddings, index_name=index_name)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `text` key. Skipping.\n",
      "Found document with no `text` key. Skipping.\n",
      "Found document with no `text` key. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To edit template content click on the default templates from the list, type Subject then type email content using the metadata buttons on the bottom of the page, click Save or to publish click Publish/Unpublish button on the top right.\n",
      "\n",
      "NOTE: To make sure clients are receiving email check the publication status on the top left corner beside the template name shown as gray box.\n",
      "\n",
      "To delete a template click on the trash icon in the top right of the template box.\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"\"\"what is The Myth of Prometheus in Literary Texts \"\"\")\n",
    "print(format_docs(retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `text` key. Skipping.\n",
      "Found document with no `text` key. Skipping.\n",
      "Found document with no `text` key. Skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'how to creat a client',\n",
       " 'result': 'To create an individual client, follow these steps:\\n\\n1. Go to the clients module from the left menu bar and select the \"individual\" tab.\\n2. Click the \"+Add\" button.\\n3. Complete the online form that appears. You will need to select one of the four client types: Primary, Dependent, Group, or Group Member.\\n4. If you are adding a Primary client, fill in the required information such as Name, Date of Birth, Email, Phone, and Address.\\n5. Click \"Save\" to save the client\\'s information.\\n6. Next, select the desired offering and add the start date.'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "\n",
    ")\n",
    "qa.invoke(\"how to creat a client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_rag_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# # Create the parallel chain\n",
    "# My_rag_chain = RunnableParallel(\n",
    "#     {\n",
    "#         \"context\": retriever | format_docs,\n",
    "#         \"question\": RunnablePassthrough()\n",
    "#     }\n",
    "# ) | custom_rag_template | llm | StrOutputParser()\n",
    "\n",
    "# ## My chain : Retriever(Pinecone) | custom_rag_template(prompt) | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `text` key. Skipping.\n",
      "Found document with no `text` key. Skipping.\n",
      "Found document with no `text` key. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an individual client, follow these steps:\n",
      "\n",
      "1. Go to the clients module from the left menu bar and select the \"individual\" tab.\n",
      "2. Click the \"+Add\" button.\n",
      "3. You will be presented with an online form to complete. There are four different client types: Primary, Dependent, Group, and Group Member.\n",
      "4. Select \"Primary\" and fill in the required information such as Name, Date of Birth, Email, Phone, and Address.\n",
      "5. Click \"Save.\"\n",
      "6. Next, select the desired offering and add the start date.\n"
     ]
    }
   ],
   "source": [
    "keyword = \"How to create a client?\"\n",
    "template = \"\"\"You are an expert LLM assistant specialized in answering questions based solely on the information provided in the uploaded documents (PDF, DOCX, or TXT formats). Use only the information from the documents to respond accurately and clearly to each question.\n",
    "\n",
    "Guidelines:\n",
    "1. Avoid using outside knowledge or assumptions. Stick strictly to the content in the documents.\n",
    "2. If the answer is not found in the uploaded documents, state, \"The answer is not specifically mentioned in the provided documents.\"\n",
    "3. Avoid using outside knowledge or assumptions. Stick strictly to the content in the documents.\n",
    "4. Maintain a professional and helpful tone thinking you are giving service to the customer for their documents.\n",
    "5. Answer for normal conversation questions like \"Hi\", \"Hey\", \"Hello\", \"How are you?\", and many others with the answer: \"Hello, How can I assist you?\".\n",
    "6. If the question is on \"summarize\" or \"summarization\", then summarize the documents to (1/4)th the size of the original documents.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(question=keyword, context=format_docs(retrieved_docs))\n",
    "\n",
    "# Create the LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# Set up the retrieval QA with custom response for missing answers\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True  # Ensures that source documents are returned\n",
    ")\n",
    "\n",
    "# Wrapper function to handle missing answers\n",
    "def get_answer(keyword):\n",
    "    response = qa.invoke(keyword)\n",
    "    \n",
    "    # Check if the response contains valid data\n",
    "    result = response.get('result', '')\n",
    "    if \"The answer is not specifically mentioned\" in result or not result.strip():\n",
    "        return \"Precise answer not found in documents, try another prompt.\"\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "# Test the response with the keyword quer\n",
    "final_answer = get_answer(\"how to create a client\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"test-2\",\n",
       "    \"dimension\": 1536,\n",
       "    \"metric\": \"dotproduct\",\n",
       "    \"host\": \"test-2-5vwf04k.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"deletion_protection\": \"disabled\"\n",
       "}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.describe_index(\"test-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found document with no `text` key. Skipping.\n",
      "Found document with no `text` key. Skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'how to modify a membership',\n",
       " 'result': \"To modify a membership, follow these steps:\\n\\n1. **Go to the clients module** from the left menu bar.\\n2. **Find the client** you want to update membership for.\\n3. **Click on the ellipses** (three dots) next to the client's name and select **modify membership**.\\n4. In the **modify membership** popup, you can view all currently active memberships as well as past inactive memberships.\\n5. To **change or update a membership**, specify a new start date for the desired benefit.\\n6. If you want to **add an additional membership**, click **+Add** on the modify membership popup, select the membership from the list, choose between Public/Private bundle (Private can only be chosen/seen by admin), add the start date of the new membership, and then select **Continue** and **Confirm**.\\n7. To **cancel an existing membership**, you can also do this from the modify membership section by selecting the option to cancel and add an end date.\\n\\nNote: The modify membership section is only visible once the membership is in approved status.\",\n",
       " 'source_documents': [Document(id='ceb21f82-51a3-491d-b867-9ca73fc7f9d4', metadata={'source': 'data/Clients Module .docx'}, page_content='Modify membership. The modify membership section is only visible once membership is in approved status. This provides a comprehensive explanation of the membership, including fees, benefit dates, and options to either complete or cancel and add an end date. Here, you can view all currently active memberships as well as past inactive memberships. Additionally, you have the flexibility to change or update a membership by specifying a new start date for the desired benefit. \\n\\nTo add additional membership: \\n\\nGo to the clients module from the left menu bar\\n\\nFind the client you want to update membership for\\n\\nClick on ellipses > modify membership\\n\\nClick +Add on the modify membership popup\\n\\nSelect membership from the list\\n\\nClick on drop down to switch to Public/Private bundle (Private can only be chosen/seen by admin).\\n\\nAdd start date of new membership.\\n\\nSelect Continue.\\n\\nConfirm.\\n\\nTo cancel existing membership \\n\\nGo to the clients module from the left menu bar'),\n",
       "  Document(id='09551b27-1e32-4475-b3cf-a3b69bab6594', metadata={'source': 'data/Clients Module .docx'}, page_content='Modify membership. The modify membership section is only visible once membership is in approved status. This provides a comprehensive explanation of the membership, including fees, benefit dates, and options to either complete or cancel and add an end date. Here, you can view all currently active memberships as well as past inactive memberships. Additionally, you have the flexibility to change or update a membership by specifying a new start date for the desired benefit. \\n\\nTo add additional membership: \\n\\nGo to the clients module from the left menu bar\\n\\nFind the client you want to update membership for\\n\\nClick on ellipses > modify membership\\n\\nClick +Add on the modify membership popup\\n\\nSelect membership from the list\\n\\nClick on drop down to switch to Public/Private bundle (Private can only be chosen/seen by admin).\\n\\nAdd start date of new membership.\\n\\nSelect Continue.\\n\\nConfirm.\\n\\nTo cancel existing membership \\n\\nGo to the clients module from the left menu bar')]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke(\"how to modify a membership\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to delete all chunks of a document by prefix\n",
    "# def delete_document_by_prefix(doc_id_prefix):\n",
    "#     chunk_ids = list_document_chunks(doc_id_prefix)\n",
    "#     if chunk_ids:\n",
    "#         index.delete(ids=chunk_ids, namespace=\"\")\n",
    "#         print(f\"Deleted {len(chunk_ids)} chunks for document: {doc_id_prefix}\")\n",
    "#     else:\n",
    "#         print(f\"No chunks found for document: {doc_id_prefix}\")\n",
    "\n",
    "# # Example usage\n",
    "# # Upload documents from the directory\n",
    "# upload_documents(directory, embeddings, index)\n",
    "\n",
    "# # List all chunk IDs for a document\n",
    "# doc_id_prefix = \"cricket_rules\"  # Replace with your actual document name\n",
    "# chunks = list_document_chunks(doc_id_prefix)\n",
    "# print(f\"Chunks for document '{doc_id_prefix}': {chunks}\")\n",
    "\n",
    "# # Delete all chunks for a document\n",
    "# delete_document_by_prefix(doc_id_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index.delete(delete_all=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
