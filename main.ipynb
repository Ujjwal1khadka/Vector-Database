{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the Pinecone and LangChain libraries required for the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \\\n",
    "#   langchain_community \\\n",
    "#   langchain_pinecone \\\n",
    "#   langchain_openai \\\n",
    "#   unstructured \\\n",
    "#   langchain-text-splitters \\\n",
    "#   pinecone-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Langchain and Pinecone Modules from the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import (\n",
    "    PineconeHybridSearchRetriever)\n",
    "from pinecone import ServerlessSpec\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pinecone\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in 'data' directory: ['Appointments Module ✓.docx', 'Comms Module ✓.docx', 'Clients - Cases Module ✓.docx', 'Clients Module ✓.docx', 'Retrieval Augmented Generation (RAG) for Everyone.docx', 'Leads Module ✓.docx', 'Activity Logs Module ✓.docx', 'Settings Module.docx', 'Reports Module ✓.docx', 'Cases Module ✓.docx', 'Clients - Comms Module ✓.docx', 'Clients - Billings Module ✓.docx', 'Virtual Visits Module ✓.docx', 'Calendar Module ✓.docx', 'Files Module ✓.docx', ' Billings Module ✓.docx', 'Clients - Clients Module ✓.docx']\n"
     ]
    }
   ],
   "source": [
    "# List all files in the 'data' directory\n",
    "print(\"Files in 'data' directory:\", os.listdir('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = 'data'\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  docs = loader.load()\n",
    "  return docs\n",
    "\n",
    "docs = load_docs(directory)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17 documents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(docs)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language ToolKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import nltk\n",
    "#  nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Keys Verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch API keys from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "# Set the environment variables\n",
    "if openai_api_key:\n",
    "    os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
    "\n",
    "#Verify that the keys are loaded\n",
    "#print(f\"OpenAI API Key: {os.environ.get('OPENAI_API_KEY')}\")\n",
    "#print(f\"Pinecone API Key: {os.environ.get('PINECONE_API_KEY')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index the data in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_serverless = True  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Embeddings Model (Dense Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"HUGGINGFACEHUB_API_TOKEN\"\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",  #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    ")\n",
    "embeddings\n",
    "\n",
    "index_name = \"test-2\"\n",
    "\n",
    "chunk_size = 1000  \n",
    "chunk_overlap = 200  \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # Assume you have the 'docs' variable which is your original list of documents\n",
    "\n",
    "# # Initialize the RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# # Split the documents\n",
    "# split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# # Extract metadata from each document\n",
    "# # metadata = [doc.metadata for doc in split_docs]\n",
    "\n",
    "# # Convert the metadata into a pandas DataFrame\n",
    "# df_metadata = pd.DataFrame(metadata)\n",
    "\n",
    "# # Print the DataFrame to see the result\n",
    "# #print(df_metadata)\n",
    "\n",
    "# # Check the columns in the DataFrame\n",
    "# print(df_metadata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_metadata.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "# # Initialize the BM25Encoder\n",
    "# bm25 = BM25Encoder()\n",
    "\n",
    "# # Assuming df_metadata is your DataFrame containing the 'productDisplayName' column\n",
    "# encode = df_metadata['source'].tolist()\n",
    "\n",
    "# # Fit the BM25 model on the productDisplayNames\n",
    "# bm25.fit(encode)\n",
    "\n",
    "# # Create lists to store the results\n",
    "# encoded_queries = []\n",
    "# encoded_documents = []\n",
    "\n",
    "# # Loop through each productDisplayName\n",
    "# for name in encode:\n",
    "#     query_encoding = bm25.encode_queries(name)\n",
    "#     document_encoding = bm25.encode_documents(name)\n",
    "    \n",
    "#     encoded_queries.append(query_encoding)\n",
    "#     encoded_documents.append(document_encoding)\n",
    "\n",
    "# # Optionally, you can convert the results into DataFrames for easier handling\n",
    "# df_encoded_queries = pd.DataFrame(encoded_queries)\n",
    "# df_encoded_documents = pd.DataFrame(encoded_documents)\n",
    "\n",
    "# # Print the results\n",
    "# #print(\"Encoded Queries:\")\n",
    "# #print(df_encoded_queries.head())\n",
    "\n",
    "# #print(\"Encoded Documents:\")\n",
    "# #print(df_encoded_documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = PineconeHybridSearchRetriever(\n",
    "#     embeddings=embeddings, sparse_encoder=bm25, index=index_name\n",
    "# )\n",
    "# retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"test-2\"\n",
    "vectorstore = PineconeVectorStore.from_documents(split_docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\" Hello\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x3366d5070>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs = {\"k\":5})\n",
    "retriever.get_relevant_documents(query)\n",
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Chat Model as GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True  # This will return source documents in the response\n",
    "\n",
    ")\n",
    "\n",
    "#qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Post-process the output\n",
    "# response = qa.invoke(query)\n",
    "# # result = response.get('result', 'No result found')\n",
    "# source_documents = response.get('source_documents', 'No source documents available')\n",
    "# source_info = response['source_documents']  \n",
    "# print(f\"Response: {response['result']} (Source: {source_info})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query and Response (with Pinecone and without Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\" According to the Constitution of the Kingdom of Nepal 1990, there is a\n",
    "# provision for electing how members to the House of\n",
    "# Representatives.\"\"\"\n",
    "\n",
    "# # Send each query to the LLM twice, first with relevant knowledge from Pincone \n",
    "# # and then without any additional knowledge.\n",
    "# print(\"Response \\n\")\n",
    "# print(\"Chat with Pinecone:\")\n",
    "# print(qa.invoke(query).get(\"result\"))\n",
    "# #print(\"\\nChat with GPT-4o:\")\n",
    "# #print(llm.invoke(query).content)\n",
    "# # Combine the two responses for clarity\n",
    "# #print(\"\\nCombined Response (Pinecone + GPT-4o):\")\n",
    "# #combined_response = f\"Pinecone Response: {\"Chat with Pinecone:\"}\\nGPT-4o Response: {\"\\nChat with GPT-4o:\"}\"\n",
    "# #print(combined_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "#print(format_docs(retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an expert LLM assistant specialized in answering questions based solely on the information provided in the uploaded documents (PDF, DOCX, or TXT formats). Use only the information from the documents to respond accurately and clearly to each question.\n",
    "\n",
    "Guidelines:\n",
    "1. Provide concise and informative answers.\n",
    "2. If the answer is not found in the uploaded documents, state, \"The answer is not available in the provided documents.\"\n",
    "3. Avoid using outside knowledge or assumptions. Stick strictly to the content in the documents.\n",
    "4. Maintain a professional and helpful tone.\n",
    "5. Answer for normal conversation question like \"Hi\", \"Hey\", \"Hello\", \"How are you\", and many others questions with answer \"Hello, How can I assist you?\".\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = template.format(question = query, context =  format_docs(retrieved_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "custom_rag_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the parallel chain\n",
    "My_rag_chain = RunnableParallel(\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ") | custom_rag_template | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "## My chain : Retriever(Pinecone) | custom_rag_template(prompt) | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query and Response (with Pinecone and without Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with your Documents:\n",
      "The persons preparing the Resource Manual on Electoral Systems in Nepal are Mr. Chiniya Narayan Shrestha, Mr. Bipul Neupane, Mr. Lok Darshan Pandit, Mr. Yamlal Adhikari, Mr. Rishiram Pangeni, and Mr. Madhusudan Pudasaini. They are all employees of the Election Commission, with titles ranging from Under Secretary to Computer Operator.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\" Persons preparing Resource Manual\n",
    "on Electoral Systems in Nepal?\"\"\"\n",
    "\n",
    "print(\"Chat with your Documents:\")\n",
    "print(My_rag_chain.invoke(query))\n",
    "#print(\"\\nChat with GPT-4o:\")\n",
    "#print(llm.invoke(query).content)\n",
    "# Combine the two responses for clarity\n",
    "#print(\"\\nCombined Response (Pinecone + GPT-4o):\")\n",
    "#combined_response = f\"Pinecone Response: {\"Chat with Pinecone:\"}\\nGPT-4o Response: {\"\\nChat with GPT-4o:\"}\"\n",
    "#print(combined_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def llm_response(query, memory = None):\n",
    "    return My_rag_chain.invoke(query)\n",
    "\n",
    "rag_demo = gr.ChatInterface(\n",
    "    llm_response, \n",
    "    title=\"RAG demo\",\n",
    "    chatbot=gr.Chatbot(height=300),\n",
    "    textbox=gr.Textbox(placeholder=\"Enter query here\", scale=5),\n",
    "    examples=[\"Hello\"],\n",
    "    retry_btn=gr.Button(\"Retry\"),\n",
    "    clear_btn=gr.Button(\"Clear\"),\n",
    "    undo_btn=gr.Button(\"Undo\"),\n",
    "    submit_btn=gr.Button(\"Submit\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tenants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (41802210.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[58], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    #print(f\"  User: {user}\")\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Define 25 tenants, each with 50 users\n",
    "tenants = {f\"tenant_{i}\": [f\"user_{i}_{j}\" for j in range(50)] for i in range(1, 26)}\n",
    "\n",
    "# Print out the tenants and their users\n",
    "for tenant_id, users in tenants.items():\n",
    "    #print(f\"Tenant ID: {tenant_id}\")\n",
    "    for user in users:\n",
    "        #print(f\"  User: {user}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder, Upload Docs, Delete Docs, Isolation Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User folders created successfully.\n"
     ]
    }
   ],
   "source": [
    "# User Folders\n",
    "def create_user_folders(base_path, tenants):\n",
    "    \"\"\"Create folders for each tenant and their users.\"\"\"\n",
    "    for tenant_id, users in tenants.items():\n",
    "        tenant_path = os.path.join(base_path, tenant_id)\n",
    "        os.makedirs(tenant_path, exist_ok=True)\n",
    "        for user in users:\n",
    "            user_path = os.path.join(tenant_path, user)\n",
    "            os.makedirs(user_path, exist_ok=True)\n",
    "    print(\"User folders created successfully.\")\n",
    "\n",
    "#Uploading File with metadata \n",
    "def upload_file(base_path, tenant_id, user_id, file_path):\n",
    "    \"\"\"Upload a file and store its embedding with metadata for tenant filtering.\"\"\"\n",
    "    tenant_folder = os.path.join(base_path, tenant_id)\n",
    "    user_folder = os.path.join(tenant_folder, user_id)\n",
    "    \n",
    "    if not os.path.exists(user_folder):\n",
    "        raise ValueError(f\"User folder {user_folder} does not exist\")\n",
    "    \n",
    "    destination = os.path.join(user_folder, os.path.basename(file_path))\n",
    "    \n",
    "    if os.path.exists(destination):\n",
    "        raise FileExistsError(f\"File {destination} already exists\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as fsrc, open(destination, 'wb') as fdst:\n",
    "            fdst.write(fsrc.read())\n",
    "        \n",
    "        # Create embeddings and store in Pinecone with tenant metadata\n",
    "        embedding = embeddings.embed_from_file(file_path)\n",
    "        metadata = {\"tenant_id\": tenant_id, \"user_id\": user_id}\n",
    "        vectorstore.add(vectors=[embedding], ids=[os.path.basename(file_path)], metadata=metadata)\n",
    "        print(f\"File {file_path} uploaded to {destination}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while uploading the file: {e}\")\n",
    "\n",
    "# Delete File\n",
    "def delete_file(base_path, tenant_id, user_id, file_name):\n",
    "    \"\"\"Delete a file from the specified tenant and user folder.\"\"\"\n",
    "    file_path = os.path.join(base_path, tenant_id, user_id, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"File {file_path} deleted successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while deleting the file: {e}\")\n",
    "    else:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "\n",
    "# Query File for Isolation\n",
    "def query_files(tenant_id, query_text):\n",
    "    \"\"\"Query files visible to the specified tenant.\"\"\"\n",
    "    query_embedding = embeddings.embed(query_text)\n",
    "    results = vectorstore.query(query=query_embedding, top_k=5, filter={\"tenant_id\": tenant_id})\n",
    "    \n",
    "    print(f\"Query results for tenant {tenant_id}:\")\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "\n",
    "##### Example for tenant and user data ######\n",
    "#####                                  ######\n",
    "tenants = {\n",
    "    'tenant_1': ['user_1_0', 'user_1_1','user_1_2','user_1_3','user_1_4',],\n",
    "    'tenant_2': ['user_2_0', 'user_2_1','user_2_2','user_2_3','user_2_4',],\n",
    "    'tenant_3': ['user_3_0', 'user_3_1','user_3_2','user_3_3','user_3_4',],\n",
    "    'tenant_4': ['user_4_0', 'user_4_1','user_4_2','user_4_3','user_4_4',],\n",
    "    'tenant_5': ['user_5_0', 'user_5_1','user_5_2','user_5_3','user_5_4',],\n",
    "    # Add more tenants and users as needed\n",
    "}\n",
    "\n",
    "# Base directory where user folders will be created\n",
    "base_path = 'file_storage'\n",
    "\n",
    "# Create folders\n",
    "create_user_folders(base_path, tenants)\n",
    "\n",
    "# Example of uploading a file\n",
    "#upload_file(base_path, 'tenant_1', 'user_1_1', 'Retrieval Augmented Generation (RAG) for Everyone.docx')\n",
    "\n",
    "# Example of deleting a file\n",
    "#delete_file(base_path, 'tenant_1', 'user_1_1', 'Retrieval Augmented Generation (RAG) for Everyone.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get an index endpoint = https://ujjwaln-rn229jx.svc.aped-4627-b74a.pinecone.io\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
