{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the Pinecone and LangChain libraries required for the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \\\n",
    "#   langchain_community \\\n",
    "#   langchain_pinecone \\\n",
    "#   langchain_openai \\\n",
    "#   unstructured \\\n",
    "#   langchain-text-splitters \\\n",
    "#   pinecone-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Langchain and Pinecone Modules from the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.retrievers import (\n",
    "    PineconeHybridSearchRetriever)\n",
    "from pinecone import ServerlessSpec\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from tqdm.auto import tqdm\n",
    "import pinecone\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in 'data' directory: ['Appointments Module ✓.docx', 'Comms Module ✓.docx', 'Clients - Cases Module ✓.docx', 'Clients Module ✓.docx', 'Retrieval Augmented Generation (RAG) for Everyone.docx', 'Leads Module ✓.docx', 'Activity Logs Module ✓.docx', 'Settings Module.docx', 'Reports Module ✓.docx', 'Cases Module ✓.docx', 'Clients - Comms Module ✓.docx', 'Clients - Billings Module ✓.docx', 'c.pdf', 'Virtual Visits Module ✓.docx', 'Calendar Module ✓.docx', 'Files Module ✓.docx', ' Billings Module ✓.docx', 'Clients - Clients Module ✓.docx']\n"
     ]
    }
   ],
   "source": [
    "# List all files in the 'data' directory\n",
    "print(\"Files in 'data' directory:\", os.listdir('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = 'data'\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  docs = loader.load()\n",
    "  return docs\n",
    "\n",
    "docs = load_docs(directory)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18 documents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(docs)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language ToolKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    " nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Keys Verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch API keys from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "# Set the environment variables\n",
    "if openai_api_key:\n",
    "    os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "if pinecone_api_key:\n",
    "    os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
    "\n",
    "#Verify that the keys are loaded\n",
    "#print(f\"OpenAI API Key: {os.environ.get('OPENAI_API_KEY')}\")\n",
    "#print(f\"Pinecone API Key: {os.environ.get('PINECONE_API_KEY')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index the data in Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_serverless = True  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Embeddings Model (Dense Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our documents into chunks\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"HUGGINGFACEHUB_API_TOKEN\"\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",  #response time is 9s  #infloat/e5-base-V2 has 3.53sec response time.\n",
    ")\n",
    "embeddings\n",
    "\n",
    "index_name = \"test-2\"\n",
    "\n",
    "# Split our documents into chunks\n",
    "chunk_size = 1000  \n",
    "chunk_overlap = 200  \n",
    "\n",
    "# Initialize the text splitter with chunk size and chunk overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Split your documents into chunks\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 (Sparse Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # Assume you have the 'docs' variable which is your original list of documents\n",
    "\n",
    "# # Initialize the RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# # Split the documents\n",
    "# split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# # Extract metadata from each document\n",
    "# # metadata = [doc.metadata for doc in split_docs]\n",
    "\n",
    "# # Convert the metadata into a pandas DataFrame\n",
    "# df_metadata = pd.DataFrame(metadata)\n",
    "\n",
    "# # Print the DataFrame to see the result\n",
    "# #print(df_metadata)\n",
    "\n",
    "# # Check the columns in the DataFrame\n",
    "# print(df_metadata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_metadata.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "# # Initialize the BM25Encoder\n",
    "# bm25 = BM25Encoder()\n",
    "\n",
    "# # Assuming df_metadata is your DataFrame containing the 'productDisplayName' column\n",
    "# encode = df_metadata['source'].tolist()\n",
    "\n",
    "# # Fit the BM25 model on the productDisplayNames\n",
    "# bm25.fit(encode)\n",
    "\n",
    "# # Create lists to store the results\n",
    "# encoded_queries = []\n",
    "# encoded_documents = []\n",
    "\n",
    "# # Loop through each productDisplayName\n",
    "# for name in encode:\n",
    "#     query_encoding = bm25.encode_queries(name)\n",
    "#     document_encoding = bm25.encode_documents(name)\n",
    "    \n",
    "#     encoded_queries.append(query_encoding)\n",
    "#     encoded_documents.append(document_encoding)\n",
    "\n",
    "# # Optionally, you can convert the results into DataFrames for easier handling\n",
    "# df_encoded_queries = pd.DataFrame(encoded_queries)\n",
    "# df_encoded_documents = pd.DataFrame(encoded_documents)\n",
    "\n",
    "# # Print the results\n",
    "# #print(\"Encoded Queries:\")\n",
    "# #print(df_encoded_queries.head())\n",
    "\n",
    "# #print(\"Encoded Documents:\")\n",
    "# #print(df_encoded_documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = PineconeHybridSearchRetriever(\n",
    "#     embeddings=embeddings, sparse_encoder=bm25, index=index_name\n",
    "# )\n",
    "# retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"test-2\"\n",
    "vectorstore = PineconeVectorStore.from_documents(split_docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Similar Documents in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenant_id = \"tenant_A\"  # Assigning metadata\n",
    "query=\"Hi, I'm Ujjwal Khadka from Novelty Technology\"  #query with metadata filters\n",
    "\n",
    "similar_docs = vectorstore.similarity_search(query, k=5, filter={\"tenant_id\": tenant_id}) #Adding an ID prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Chat Model as GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True  # This will return source documents in the response\n",
    "\n",
    ")\n",
    "\n",
    "#qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Hello Ujjwal Khadka! How can I assist you today? (Source: [Document(metadata={'source': 'data/c.pdf'}, page_content=\"Computer Operator, Election Commission\\n\\n7 Mr. Purna Man Shakya\\n\\nAdvisor, Reliance Law Firm\\n\\nVoters' Awareness Program Coordination Committee\\n\\nName\\n\\n1 Mr. Purusottam Sapkota\\n\\n2 Mr. Neel Kantha Upreti\\n\\nDesignation/Organization Secretary,\\n\\nUnder Coordinator Project Chief, Election Commission – Member\\n\\nElection Commission-\\n\\n3 Mr. Mark Wallem\\n\\nDirector NDI/Nepal - Member\\n\\n4 Ms. Anamika Rai\\n\\nProgram Officer, NDI/Nepal-Member Secretary\\n\\n47\"), Document(metadata={'source': 'data/c.pdf'}, page_content=\"Computer Operator, Election Commission\\n\\n7 Mr. Purna Man Shakya\\n\\nAdvisor, Reliance Law Firm\\n\\nVoters' Awareness Program Coordination Committee\\n\\nName\\n\\n1 Mr. Purusottam Sapkota\\n\\n2 Mr. Neel Kantha Upreti\\n\\nDesignation/Organization Secretary,\\n\\nUnder Coordinator Project Chief, Election Commission – Member\\n\\nElection Commission-\\n\\n3 Mr. Mark Wallem\\n\\nDirector NDI/Nepal - Member\\n\\n4 Ms. Anamika Rai\\n\\nProgram Officer, NDI/Nepal-Member Secretary\\n\\n47\"), Document(metadata={'source': 'data/c.pdf'}, page_content=\"Computer Operator, Election Commission\\n\\n7 Mr. Purna Man Shakya\\n\\nAdvisor, Reliance Law Firm\\n\\nVoters' Awareness Program Coordination Committee\\n\\nName\\n\\n1 Mr. Purusottam Sapkota\\n\\n2 Mr. Neel Kantha Upreti\\n\\nDesignation/Organization Secretary,\\n\\nUnder Coordinator Project Chief, Election Commission – Member\\n\\nElection Commission-\\n\\n3 Mr. Mark Wallem\\n\\nDirector NDI/Nepal - Member\\n\\n4 Ms. Anamika Rai\\n\\nProgram Officer, NDI/Nepal-Member Secretary\\n\\n47\"), Document(metadata={'source': 'data/c.pdf'}, page_content=\"Computer Operator, Election Commission\\n\\n7 Mr. Purna Man Shakya\\n\\nAdvisor, Reliance Law Firm\\n\\nVoters' Awareness Program Coordination Committee\\n\\nName\\n\\n1 Mr. Purusottam Sapkota\\n\\n2 Mr. Neel Kantha Upreti\\n\\nDesignation/Organization Secretary,\\n\\nUnder Coordinator Project Chief, Election Commission – Member\\n\\nElection Commission-\\n\\n3 Mr. Mark Wallem\\n\\nDirector NDI/Nepal - Member\\n\\n4 Ms. Anamika Rai\\n\\nProgram Officer, NDI/Nepal-Member Secretary\\n\\n47\")])\n"
     ]
    }
   ],
   "source": [
    "# Post-process the output\n",
    "response = qa.invoke(query)\n",
    "# result = response.get('result', 'No result found')\n",
    "source_documents = response.get('source_documents', 'No source documents available')\n",
    "source_info = response['source_documents']  \n",
    "print(f\"Response: {response['result']} (Source: {source_info})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query and Response (with Pinecone and without Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response \n",
      "\n",
      "Chat with Pinecone:\n",
      "To approve a membership, follow these steps:\n",
      "\n",
      "1. After creating a client (either an Individual or a group), they will be in a \"submitted\" status.\n",
      "2. Review the client's information to ensure everything is correct and meets the necessary criteria.\n",
      "3. Approve the client or group. Once approved, the client or group will move out of the \"submitted\" status.\n",
      "4. Once approved, clients will receive a receipt and a welcome email (if applicable).\n",
      "\n",
      "Note: No transactions will happen until the client or group is approved. If a client or group is declined, they will receive an email with notes, if any are provided.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\" 12. How to approve a membership????\"\"\"\n",
    "\n",
    "# Send each query to the LLM twice, first with relevant knowledge from Pincone \n",
    "# and then without any additional knowledge.\n",
    "print(\"Response \\n\")\n",
    "print(\"Chat with Pinecone:\")\n",
    "print(qa.invoke(query).get(\"result\"))\n",
    "#print(\"\\nChat with GPT-4o:\")\n",
    "#print(llm.invoke(query).content)\n",
    "# Combine the two responses for clarity\n",
    "#print(\"\\nCombined Response (Pinecone + GPT-4o):\")\n",
    "#combined_response = f\"Pinecone Response: {\"Chat with Pinecone:\"}\\nGPT-4o Response: {\"\\nChat with GPT-4o:\"}\"\n",
    "#print(combined_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to delete documents by ids\n",
    "def delete_documents(ids):\n",
    "    vectorstore.delete(ids)\n",
    "    \n",
    "\n",
    "# Deleting documents with specific ids\n",
    "ids_to_delete = [\"id1\", \"id2\"]  # Replace with actual document ids\n",
    "delete_documents(ids_to_delete)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Everything with a SequentialChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
